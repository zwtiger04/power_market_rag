# âš¡ ì „ë ¥ì‹œì¥ RAG ì‹œìŠ¤í…œ ì™„ì „ ê°€ì´ë“œ

## ğŸ“‹ ëª©ì°¨

1. [ì‹œìŠ¤í…œ ê°œìš”](#1-ì‹œìŠ¤í…œ-ê°œìš”)
2. [í”„ë¡œì íŠ¸ êµ¬ì¡°](#2-í”„ë¡œì íŠ¸-êµ¬ì¡°)
3. [í•µì‹¬ ëª¨ë“ˆ ì½”ë“œ](#3-í•µì‹¬-ëª¨ë“ˆ-ì½”ë“œ)
4. [ì„¤ì • ë° ìŠ¤í¬ë¦½íŠ¸](#4-ì„¤ì •-ë°-ìŠ¤í¬ë¦½íŠ¸)
5. [ì˜ˆì‹œ ë¬¸ì„œ](#5-ì˜ˆì‹œ-ë¬¸ì„œ)
6. [ì„¤ì¹˜ ë° ì‹¤í–‰](#6-ì„¤ì¹˜-ë°-ì‹¤í–‰)
7. [ì‚¬ìš©ë²• ê°€ì´ë“œ](#7-ì‚¬ìš©ë²•-ê°€ì´ë“œ)
8. [API ì‚¬ìš©ë²•](#8-api-ì‚¬ìš©ë²•)
9. [ë¬¸ì œí•´ê²°](#9-ë¬¸ì œí•´ê²°)
10. [ê³ ê¸‰ ì„¤ì •](#10-ê³ ê¸‰-ì„¤ì •)

---

## 1. ì‹œìŠ¤í…œ ê°œìš”

### ğŸ¯ RAG (Retrieval-Augmented Generation)ë€?

**RAG**ëŠ” "ê²€ìƒ‰ ì¦ê°• ìƒì„±"ìœ¼ë¡œ, ë‘ ë‹¨ê³„ë¡œ ì‘ë™í•©ë‹ˆë‹¤:
1. **ê²€ìƒ‰ ë‹¨ê³„**: ì§ˆë¬¸ê³¼ ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ìŠµë‹ˆë‹¤
2. **ìƒì„± ë‹¨ê³„**: ì°¾ì€ ë¬¸ì„œë¥¼ ë°”íƒ•ìœ¼ë¡œ ë‹µë³€ì„ ìƒì„±í•©ë‹ˆë‹¤

### ğŸ—ï¸ ì‹œìŠ¤í…œ ì•„í‚¤í…ì²˜

```
ğŸ“„ ë¬¸ì„œ ì…ë ¥ â†’ ğŸ”¤ í…ìŠ¤íŠ¸ ì¶”ì¶œ â†’ âœ‚ï¸ ì²­í‚¹ â†’ ğŸ§  ì„ë² ë”© â†’ ğŸ’¾ ë²¡í„°DB ì €ì¥
                                                                    â†“
ğŸ’¡ ë‹µë³€ ìƒì„± â† ğŸ“ ì»¨í…ìŠ¤íŠ¸ êµ¬ì„± â† ğŸ” ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ â† â“ ì‚¬ìš©ì ì§ˆë¬¸
```

### â­ ì£¼ìš” íŠ¹ì§•

- **ë‹¤ì–‘í•œ ê²€ìƒ‰ ë°©ì‹**: ì˜ë¯¸ì , í‚¤ì›Œë“œ, í•˜ì´ë¸Œë¦¬ë“œ, ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰
- **ì „ë ¥ì‹œì¥ íŠ¹í™”**: ì „ë¬¸ìš©ì–´ì™€ ê·œì •ì— ìµœì í™”
- **ëª¨ë“ˆí˜• êµ¬ì¡°**: ë…ë¦½ì ì´ë©´ì„œ ìœ ê¸°ì ìœ¼ë¡œ ì—°ê²°ëœ ëª¨ë“ˆë“¤
- **ì›¹ ì¸í„°í˜ì´ìŠ¤**: ì‚¬ìš©ì ì¹œí™”ì  ì›¹ UI ì œê³µ
- **API ì§€ì›**: ë‹¤ë¥¸ ì‹œìŠ¤í…œê³¼ ì‰¬ìš´ ì—°ë™

---

## 2. í”„ë¡œì íŠ¸ êµ¬ì¡°

```
power_market_rag/
â”œâ”€â”€ ğŸ“„ power_market_rag.py          # ë©”ì¸ RAG ì‹œìŠ¤í…œ
â”œâ”€â”€ ğŸ“„ demo.py                      # ë°ëª¨ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ ğŸ“„ requirements.txt             # íŒ¨í‚¤ì§€ ëª©ë¡
â”œâ”€â”€ ğŸ“„ README.md                   # ì‚¬ìš©ë²• ê°€ì´ë“œ
â”œâ”€â”€ ğŸ“„ install.sh                  # ìë™ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ ğŸ“„ run.sh                      # ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸
â”œâ”€â”€ ğŸ“„ test_basic.py               # ê¸°ë³¸ í…ŒìŠ¤íŠ¸
â”œâ”€â”€ ğŸ“‚ embeddings/                 # ì„ë² ë”© ëª¨ë“ˆ
â”‚   â”œâ”€â”€ document_processor.py       # ë¬¸ì„œ ì²˜ë¦¬
â”‚   â””â”€â”€ text_embedder.py            # í…ìŠ¤íŠ¸ ì„ë² ë”©
â”œâ”€â”€ ğŸ“‚ vector_db/                  # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
â”‚   â””â”€â”€ vector_store.py             # ChromaDB ê´€ë¦¬
â”œâ”€â”€ ğŸ“‚ retrieval/                  # ê²€ìƒ‰ ì—”ì§„
â”‚   â””â”€â”€ document_retriever.py       # ë¬¸ì„œ ê²€ìƒ‰
â”œâ”€â”€ ğŸ“‚ generation/                 # ë‹µë³€ ìƒì„±
â”‚   â””â”€â”€ answer_generator.py         # ë‹µë³€ ìƒì„±ê¸°
â”œâ”€â”€ ğŸ“‚ api/                        # ì›¹ API
â”‚   â””â”€â”€ api_server.py               # FastAPI ì„œë²„
â”œâ”€â”€ ğŸ“‚ config/                     # ì„¤ì •
â”‚   â””â”€â”€ config.yaml                 # ì‹œìŠ¤í…œ ì„¤ì •
â”œâ”€â”€ ğŸ“‚ documents/                  # ë¬¸ì„œ ì €ì¥ì†Œ
â”‚   â”œâ”€â”€ ë°œì „ê³„íš_ê°€ì´ë“œ.md
â”‚   â”œâ”€â”€ ê³„í†µìš´ì˜ê¸°ì¤€.md
â”‚   â””â”€â”€ ì „ë ¥ì‹œì¥_ê±°ë˜ì ˆì°¨.md
â”œâ”€â”€ ğŸ“‚ vector_db/                  # ë²¡í„° DB ì €ì¥ì†Œ
â”œâ”€â”€ ğŸ“‚ logs/                       # ë¡œê·¸ íŒŒì¼
â””â”€â”€ ğŸ“‚ tests/                      # í…ŒìŠ¤íŠ¸ ì½”ë“œ
    â””â”€â”€ test_rag_system.py
```

---

## 3. í•µì‹¬ ëª¨ë“ˆ ì½”ë“œ

### 3.1 ë¬¸ì„œ ì²˜ë¦¬ ëª¨ë“ˆ (embeddings/document_processor.py)

```python
"""
ë¬¸ì„œ ì²˜ë¦¬ ëª¨ë“ˆ
- PDF, í…ìŠ¤íŠ¸, Word íŒŒì¼ì„ ì½ì–´ì„œ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜
- í…ìŠ¤íŠ¸ë¥¼ ì ì ˆí•œ í¬ê¸°ë¡œ ë‚˜ëˆ„ê¸° (chunking)
"""

import os
import logging
from typing import List, Dict
from pathlib import Path

import PyPDF2
from docx import Document
import pandas as pd

class DocumentProcessor:
    """ë¬¸ì„œë¥¼ ì²˜ë¦¬í•˜ê³  í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, chunk_size: int = 1000, chunk_overlap: int = 200):
        """
        Args:
            chunk_size: í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ëŠ” í¬ê¸° (ê¸€ì ìˆ˜)
            chunk_overlap: ê²¹ì¹˜ëŠ” ë¶€ë¶„ì˜ í¬ê¸° (ê¸€ì ìˆ˜)
        """
        self.chunk_size = chunk_size
        self.chunk_overlap = chunk_overlap
        self.logger = logging.getLogger(__name__)
    
    def extract_text_from_pdf(self, file_path: str) -> str:
        """PDF íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"""
        try:
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                text = ""
                
                # ê° í˜ì´ì§€ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                for page_num in range(len(pdf_reader.pages)):
                    page = pdf_reader.pages[page_num]
                    text += page.extract_text() + "\n"
                
                return text.strip()
        except Exception as e:
            self.logger.error(f"PDF ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
            return ""
    
    def extract_text_from_docx(self, file_path: str) -> str:
        """Word íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"""
        try:
            doc = Document(file_path)
            text = ""
            
            # ê° ë¬¸ë‹¨ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
            for paragraph in doc.paragraphs:
                text += paragraph.text + "\n"
            
            return text.strip()
        except Exception as e:
            self.logger.error(f"Word íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
            return ""
    
    def extract_text_from_txt(self, file_path: str) -> str:
        """í…ìŠ¤íŠ¸ íŒŒì¼ì—ì„œ ë‚´ìš©ì„ ì½ìŠµë‹ˆë‹¤"""
        try:
            with open(file_path, 'r', encoding='utf-8') as file:
                return file.read().strip()
        except Exception as e:
            self.logger.error(f"í…ìŠ¤íŠ¸ íŒŒì¼ ì½ê¸° ì˜¤ë¥˜ {file_path}: {e}")
            return ""
    
    def process_file(self, file_path: str) -> str:
        """íŒŒì¼ í™•ì¥ìì— ë”°ë¼ ì ì ˆí•œ ë°©ë²•ìœ¼ë¡œ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤"""
        file_path = Path(file_path)
        extension = file_path.suffix.lower()
        
        if extension == '.pdf':
            return self.extract_text_from_pdf(str(file_path))
        elif extension == '.docx':
            return self.extract_text_from_docx(str(file_path))
        elif extension in ['.txt', '.md']:
            return self.extract_text_from_txt(str(file_path))
        else:
            self.logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹: {extension}")
            return ""
    
    def split_text_into_chunks(self, text: str) -> List[Dict[str, any]]:
        """ê¸´ í…ìŠ¤íŠ¸ë¥¼ ì‘ì€ ì¡°ê°ë“¤ë¡œ ë‚˜ëˆ•ë‹ˆë‹¤"""
        chunks = []
        
        # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë‚˜ëˆ„ê¸°
        sentences = text.split('. ')
        
        current_chunk = ""
        chunk_id = 0
        
        for sentence in sentences:
            # í˜„ì¬ ì¡°ê°ì— ë¬¸ì¥ì„ ì¶”ê°€í–ˆì„ ë•Œ í¬ê¸° í™•ì¸
            potential_chunk = current_chunk + sentence + ". "
            
            if len(potential_chunk) <= self.chunk_size:
                current_chunk = potential_chunk
            else:
                # í˜„ì¬ ì¡°ê°ì´ ë¹„ì–´ìˆì§€ ì•Šìœ¼ë©´ ì €ì¥
                if current_chunk.strip():
                    chunks.append({
                        'id': chunk_id,
                        'text': current_chunk.strip(),
                        'length': len(current_chunk)
                    })
                    chunk_id += 1
                
                # ìƒˆë¡œìš´ ì¡°ê° ì‹œì‘
                # ê²¹ì¹˜ëŠ” ë¶€ë¶„ ê³„ì‚°
                if self.chunk_overlap > 0 and len(current_chunk) > self.chunk_overlap:
                    overlap_text = current_chunk[-self.chunk_overlap:]
                    current_chunk = overlap_text + sentence + ". "
                else:
                    current_chunk = sentence + ". "
        
        # ë§ˆì§€ë§‰ ì¡°ê° ì €ì¥
        if current_chunk.strip():
            chunks.append({
                'id': chunk_id,
                'text': current_chunk.strip(),
                'length': len(current_chunk)
            })
        
        return chunks
    
    def process_documents_from_directory(self, directory_path: str) -> List[Dict[str, any]]:
        """ë””ë ‰í† ë¦¬ì— ìˆëŠ” ëª¨ë“  ë¬¸ì„œë¥¼ ì²˜ë¦¬í•©ë‹ˆë‹¤"""
        all_chunks = []
        directory = Path(directory_path)
        
        if not directory.exists():
            self.logger.error(f"ë””ë ‰í† ë¦¬ê°€ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {directory_path}")
            return all_chunks
        
        supported_extensions = ['.pdf', '.docx', '.txt', '.md']
        
        for file_path in directory.rglob('*'):
            if file_path.is_file() and file_path.suffix.lower() in supported_extensions:
                self.logger.info(f"ì²˜ë¦¬ ì¤‘ì¸ íŒŒì¼: {file_path}")
                
                # íŒŒì¼ì—ì„œ í…ìŠ¤íŠ¸ ì¶”ì¶œ
                text = self.process_file(str(file_path))
                
                if text:
                    # í…ìŠ¤íŠ¸ë¥¼ ì¡°ê°ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
                    chunks = self.split_text_into_chunks(text)
                    
                    # ë©”íƒ€ë°ì´í„° ì¶”ê°€
                    for chunk in chunks:
                        chunk.update({
                            'source_file': str(file_path),
                            'file_name': file_path.name,
                            'file_type': file_path.suffix.lower()
                        })
                    
                    all_chunks.extend(chunks)
                    self.logger.info(f"íŒŒì¼ {file_path.name}ì—ì„œ {len(chunks)}ê°œ ì¡°ê° ìƒì„±")
        
        self.logger.info(f"ì´ {len(all_chunks)}ê°œì˜ í…ìŠ¤íŠ¸ ì¡°ê°ì´ ì²˜ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤")
        return all_chunks
```

### 3.2 í…ìŠ¤íŠ¸ ì„ë² ë”© ëª¨ë“ˆ (embeddings/text_embedder.py)

```python
"""
ì„ë² ë”© ëª¨ë“ˆ
- í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°(ìˆ«ì ë°°ì—´)ë¡œ ë³€í™˜
- í•œêµ­ì–´ì™€ ì „ë ¥ì‹œì¥ ì „ë¬¸ìš©ì–´ë¥¼ ì˜ ì´í•´í•˜ëŠ” ëª¨ë¸ ì‚¬ìš©
"""

import logging
from typing import List, Dict, Union
import numpy as np
from sentence_transformers import SentenceTransformer
import torch

class TextEmbedder:
    """í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        """
        Args:
            model_name: ì‚¬ìš©í•  ì„ë² ë”© ëª¨ë¸ ì´ë¦„
                      - paraphrase-multilingual-MiniLM-L12-v2: ë‹¤êµ­ì–´ ì§€ì›, ê²½ëŸ‰í™”
                      - paraphrase-multilingual-mpnet-base-v2: ë” ë†’ì€ ì„±ëŠ¥, ë¬´ê±°ì›€
        """
        self.logger = logging.getLogger(__name__)
        self.model_name = model_name
        
        try:
            self.logger.info(f"ì„ë² ë”© ëª¨ë¸ ë¡œë”© ì¤‘: {model_name}")
            self.model = SentenceTransformer(model_name)
            self.embedding_dimension = self.model.get_sentence_embedding_dimension()
            self.logger.info(f"ëª¨ë¸ ë¡œë”© ì™„ë£Œ. ì„ë² ë”© ì°¨ì›: {self.embedding_dimension}")
        except Exception as e:
            self.logger.error(f"ëª¨ë¸ ë¡œë”© ì‹¤íŒ¨: {e}")
            raise
    
    def encode_text(self, text: str) -> np.ndarray:
        """ë‹¨ì¼ í…ìŠ¤íŠ¸ë¥¼ ë²¡í„°ë¡œ ë³€í™˜"""
        try:
            # í…ìŠ¤íŠ¸ê°€ ë¹„ì–´ìˆëŠ”ì§€ í™•ì¸
            if not text or not text.strip():
                self.logger.warning("ë¹ˆ í…ìŠ¤íŠ¸ê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤")
                return np.zeros(self.embedding_dimension)
            
            # ì„ë² ë”© ìƒì„±
            embedding = self.model.encode(text, convert_to_numpy=True)
            return embedding
            
        except Exception as e:
            self.logger.error(f"í…ìŠ¤íŠ¸ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            return np.zeros(self.embedding_dimension)
    
    def encode_batch(self, texts: List[str], batch_size: int = 32) -> np.ndarray:
        """ì—¬ëŸ¬ í…ìŠ¤íŠ¸ë¥¼ í•œë²ˆì— ë²¡í„°ë¡œ ë³€í™˜ (ë” íš¨ìœ¨ì )"""
        try:
            if not texts:
                self.logger.warning("ë¹ˆ í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸ê°€ ì…ë ¥ë˜ì—ˆìŠµë‹ˆë‹¤")
                return np.array([])
            
            # ë¹ˆ í…ìŠ¤íŠ¸ í•„í„°ë§
            valid_texts = [text for text in texts if text and text.strip()]
            
            if not valid_texts:
                self.logger.warning("ìœ íš¨í•œ í…ìŠ¤íŠ¸ê°€ ì—†ìŠµë‹ˆë‹¤")
                return np.zeros((len(texts), self.embedding_dimension))
            
            self.logger.info(f"{len(valid_texts)}ê°œ í…ìŠ¤íŠ¸ ë°°ì¹˜ ì„ë² ë”© ì‹œì‘")
            
            # ë°°ì¹˜ë¡œ ì„ë² ë”© ìƒì„±
            embeddings = self.model.encode(
                valid_texts,
                batch_size=batch_size,
                convert_to_numpy=True,
                show_progress_bar=True
            )
            
            self.logger.info("ë°°ì¹˜ ì„ë² ë”© ì™„ë£Œ")
            return embeddings
            
        except Exception as e:
            self.logger.error(f"ë°°ì¹˜ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            return np.zeros((len(texts), self.embedding_dimension))
    
    def encode_documents(self, documents: List[Dict[str, any]]) -> List[Dict[str, any]]:
        """ë¬¸ì„œ ì¡°ê°ë“¤ì„ ì„ë² ë”©í•˜ê³  ë©”íƒ€ë°ì´í„°ì™€ í•¨ê»˜ ë°˜í™˜"""
        try:
            if not documents:
                self.logger.warning("ì²˜ë¦¬í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            # í…ìŠ¤íŠ¸ë§Œ ì¶”ì¶œ
            texts = [doc.get('text', '') for doc in documents]
            
            # ë°°ì¹˜ ì„ë² ë”©
            embeddings = self.encode_batch(texts)
            
            # ì›ë³¸ ë¬¸ì„œì— ì„ë² ë”© ì¶”ê°€
            embedded_documents = []
            for i, doc in enumerate(documents):
                embedded_doc = doc.copy()
                embedded_doc['embedding'] = embeddings[i] if i < len(embeddings) else np.zeros(self.embedding_dimension)
                embedded_doc['embedding_model'] = self.model_name
                embedded_documents.append(embedded_doc)
            
            self.logger.info(f"{len(embedded_documents)}ê°œ ë¬¸ì„œ ì„ë² ë”© ì™„ë£Œ")
            return embedded_documents
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì„ë² ë”© ì‹¤íŒ¨: {e}")
            return documents

class PowerMarketEmbedder(TextEmbedder):
    """ì „ë ¥ì‹œì¥ íŠ¹í™” ì„ë² ë”© í´ë˜ìŠ¤"""
    
    def __init__(self, model_name: str = "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"):
        super().__init__(model_name)
        
        # ì „ë ¥ì‹œì¥ ì „ë¬¸ìš©ì–´ ì‚¬ì „
        self.power_market_terms = {
            "ë°œì „ê³„íš": "power generation plan",
            "ê³„í†µìš´ì˜": "power system operation", 
            "ì „ë ¥ê±°ë˜": "electricity trading",
            "ì‹œì¥ìš´ì˜": "market operation",
            "ì˜ˆë¹„ë ¥": "reserve power",
            "ì†¡ì „ì œì•½": "transmission constraint",
            "í•˜ë£¨ì „ì‹œì¥": "day-ahead market",
            "ì‹¤ì‹œê°„ì‹œì¥": "real-time market",
            "ê³„í†µí•œê³„ê°€ê²©": "system marginal price",
            "ê¸‰ì „ì§€ì‹œ": "dispatch instruction"
        }
    
    def preprocess_power_market_text(self, text: str) -> str:
        """ì „ë ¥ì‹œì¥ í…ìŠ¤íŠ¸ ì „ì²˜ë¦¬"""
        # ì „ë¬¸ìš©ì–´ ì •ê·œí™”
        processed_text = text
        
        # íŠ¹ìˆ˜ ë¬¸ì ì •ë¦¬
        processed_text = processed_text.replace('\n', ' ').replace('\t', ' ')
        
        # ì—°ì†ëœ ê³µë°± ì œê±°
        import re
        processed_text = re.sub(r'\s+', ' ', processed_text).strip()
        
        return processed_text
    
    def encode_text(self, text: str) -> np.ndarray:
        """ì „ë ¥ì‹œì¥ íŠ¹í™” í…ìŠ¤íŠ¸ ì„ë² ë”©"""
        # ì „ì²˜ë¦¬ í›„ ì„ë² ë”©
        processed_text = self.preprocess_power_market_text(text)
        return super().encode_text(processed_text)
```

### 3.3 ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆ (vector_db/vector_store.py)

```python
"""
ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ëª¨ë“ˆ
- ì„ë² ë”©ëœ ë²¡í„°ë“¤ì„ ì €ì¥í•˜ê³  ê²€ìƒ‰
- ChromaDBë¥¼ ì‚¬ìš©í•œ ë²¡í„° ì €ì¥ì†Œ êµ¬í˜„
"""

import logging
import os
from typing import List, Dict, Optional, Union
import numpy as np
import chromadb
from chromadb.config import Settings
import uuid
import json

class VectorDatabase:
    """ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 db_path: str = "./vector_db",
                 collection_name: str = "power_market_docs"):
        """
        Args:
            db_path: ë°ì´í„°ë² ì´ìŠ¤ ì €ì¥ ê²½ë¡œ
            collection_name: ì»¬ë ‰ì…˜(í…Œì´ë¸”) ì´ë¦„
        """
        self.logger = logging.getLogger(__name__)
        self.db_path = db_path
        self.collection_name = collection_name
        
        # ë°ì´í„°ë² ì´ìŠ¤ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(db_path, exist_ok=True)
        
        try:
            # ChromaDB í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
            self.client = chromadb.PersistentClient(
                path=db_path,
                settings=Settings(
                    anonymized_telemetry=False  # í…”ë ˆë©”íŠ¸ë¦¬ ë¹„í™œì„±í™”
                )
            )
            
            # ì»¬ë ‰ì…˜ ìƒì„± ë˜ëŠ” ê°€ì ¸ì˜¤ê¸°
            self.collection = self.client.get_or_create_collection(
                name=collection_name,
                metadata={"description": "ì „ë ¥ì‹œì¥ ë¬¸ì„œ ë²¡í„° ì €ì¥ì†Œ"}
            )
            
            self.logger.info(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì™„ë£Œ: {db_path}")
            
        except Exception as e:
            self.logger.error(f"ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
            raise
    
    def add_documents(self, documents: List[Dict[str, any]]) -> bool:
        """ë¬¸ì„œë“¤ì„ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€"""
        try:
            if not documents:
                self.logger.warning("ì¶”ê°€í•  ë¬¸ì„œê°€ ì—†ìŠµë‹ˆë‹¤")
                return False
            
            # ë°ì´í„° ì¤€ë¹„
            ids = []
            embeddings = []
            metadatas = []
            documents_text = []
            
            for doc in documents:
                # ê³ ìœ  ID ìƒì„± (íŒŒì¼ëª… + ì¡°ê° ID)
                doc_id = f"{doc.get('file_name', 'unknown')}_{doc.get('id', uuid.uuid4())}"
                ids.append(doc_id)
                
                # ì„ë² ë”© ë²¡í„°
                embedding = doc.get('embedding', [])
                if isinstance(embedding, np.ndarray):
                    embedding = embedding.tolist()
                embeddings.append(embedding)
                
                # ë©”íƒ€ë°ì´í„° (ì„ë² ë”© ì œì™¸í•œ ëª¨ë“  ì •ë³´)
                metadata = {k: v for k, v in doc.items() 
                           if k not in ['embedding', 'text'] and v is not None}
                
                # ë©”íƒ€ë°ì´í„° ê°’ë“¤ì„ ë¬¸ìì—´ë¡œ ë³€í™˜ (ChromaDB ìš”êµ¬ì‚¬í•­)
                for key, value in metadata.items():
                    if not isinstance(value, (str, int, float, bool)):
                        metadata[key] = str(value)
                
                metadatas.append(metadata)
                
                # ë¬¸ì„œ í…ìŠ¤íŠ¸
                documents_text.append(doc.get('text', ''))
            
            # ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€
            self.collection.add(
                ids=ids,
                embeddings=embeddings,
                metadatas=metadatas,
                documents=documents_text
            )
            
            self.logger.info(f"{len(documents)}ê°œ ë¬¸ì„œë¥¼ ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì— ì¶”ê°€í–ˆìŠµë‹ˆë‹¤")
            return True
            
        except Exception as e:
            self.logger.error(f"ë¬¸ì„œ ì¶”ê°€ ì‹¤íŒ¨: {e}")
            return False
    
    def search_similar(self, 
                      query_embedding: Union[np.ndarray, List[float]], 
                      top_k: int = 5,
                      where: Optional[Dict] = None) -> List[Dict[str, any]]:
        """ìœ ì‚¬í•œ ë¬¸ì„œ ê²€ìƒ‰"""
        try:
            # numpy ë°°ì—´ì„ ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜
            if isinstance(query_embedding, np.ndarray):
                query_embedding = query_embedding.tolist()
            
            # ê²€ìƒ‰ ì‹¤í–‰
            results = self.collection.query(
                query_embeddings=[query_embedding],
                n_results=top_k,
                where=where  # ë©”íƒ€ë°ì´í„° í•„í„°ë§
            )
            
            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_results = []
            
            if results['ids'] and len(results['ids'][0]) > 0:
                for i in range(len(results['ids'][0])):
                    result = {
                        'id': results['ids'][0][i],
                        'text': results['documents'][0][i] if results['documents'] else '',
                        'metadata': results['metadatas'][0][i] if results['metadatas'] else {},
                        'distance': results['distances'][0][i] if results['distances'] else 0.0,
                        'similarity': 1 - results['distances'][0][i] if results['distances'] else 1.0
                    }
                    formatted_results.append(result)
            
            self.logger.info(f"{len(formatted_results)}ê°œì˜ ìœ ì‚¬ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤")
            return formatted_results
            
        except Exception as e:
            self.logger.error(f"ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def search_by_text(self, 
                      query_text: str, 
                      top_k: int = 5,
                      where: Optional[Dict] = None) -> List[Dict[str, any]]:
        """í…ìŠ¤íŠ¸ë¡œ ì§ì ‘ ê²€ìƒ‰ (ChromaDBì˜ í…ìŠ¤íŠ¸ ê²€ìƒ‰ ê¸°ëŠ¥ ì‚¬ìš©)"""
        try:
            results = self.collection.query(
                query_texts=[query_text],
                n_results=top_k,
                where=where
            )
            
            # ê²°ê³¼ í¬ë§·íŒ…
            formatted_results = []
            
            if results['ids'] and len(results['ids'][0]) > 0:
                for i in range(len(results['ids'][0])):
                    result = {
                        'id': results['ids'][0][i],
                        'text': results['documents'][0][i] if results['documents'] else '',
                        'metadata': results['metadatas'][0][i] if results['metadatas'] else {},
                        'distance': results['distances'][0][i] if results['distances'] else 0.0,
                        'similarity': 1 - results['distances'][0][i] if results['distances'] else 1.0
                    }
                    formatted_results.append(result)
            
            self.logger.info(f"í…ìŠ¤íŠ¸ '{query_text}'ë¡œ {len(formatted_results)}ê°œ ë¬¸ì„œë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤")
            return formatted_results
            
        except Exception as e:
            self.logger.error(f"í…ìŠ¤íŠ¸ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_collection_stats(self) -> Dict[str, any]:
        """ì»¬ë ‰ì…˜ í†µê³„ ì •ë³´"""
        try:
            count = self.collection.count()
            
            return {
                'document_count': count,
                'collection_name': self.collection_name,
                'db_path': self.db_path
            }
            
        except Exception as e:
            self.logger.error(f"í†µê³„ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return {}
```

### 3.4 ë¬¸ì„œ ê²€ìƒ‰ ëª¨ë“ˆ (retrieval/document_retriever.py)

```python
"""
ê²€ìƒ‰(Retrieval) ëª¨ë“ˆ
- ì‚¬ìš©ì ì§ˆë¬¸ì„ ë°›ì•„ì„œ ê´€ë ¨ ë¬¸ì„œë¥¼ ì°¾ëŠ” ì—­í• 
- ë‹¤ì–‘í•œ ê²€ìƒ‰ ì „ëµ êµ¬í˜„ (ë²¡í„° ê²€ìƒ‰, í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ë“±)
"""

import logging
from typing import List, Dict, Optional, Union
import numpy as np
import re
from dataclasses import dataclass

@dataclass
class SearchResult:
    """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    id: str
    text: str
    metadata: Dict
    similarity: float
    source_file: str
    relevance_score: float = 0.0

class DocumentRetriever:
    """ë¬¸ì„œ ê²€ìƒ‰ ì—”ì§„ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 vector_db,
                 text_embedder,
                 top_k: int = 5,
                 similarity_threshold: float = 0.7):
        """
        Args:
            vector_db: ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì¸ìŠ¤í„´ìŠ¤
            text_embedder: í…ìŠ¤íŠ¸ ì„ë² ë”© ì¸ìŠ¤í„´ìŠ¤
            top_k: ë°˜í™˜í•  ìµœëŒ€ ê²°ê³¼ ìˆ˜
            similarity_threshold: ìœ ì‚¬ë„ ì„ê³„ê°’
        """
        self.logger = logging.getLogger(__name__)
        self.vector_db = vector_db
        self.text_embedder = text_embedder
        self.top_k = top_k
        self.similarity_threshold = similarity_threshold
        
        # ì „ë ¥ì‹œì¥ í‚¤ì›Œë“œ ê°€ì¤‘ì¹˜
        self.power_market_keywords = {
            "ë°œì „ê³„íš": 1.5,
            "ê³„í†µìš´ì˜": 1.5,
            "ì „ë ¥ê±°ë˜": 1.4,
            "ì‹œì¥ìš´ì˜": 1.4,
            "ì˜ˆë¹„ë ¥": 1.3,
            "ì†¡ì „ì œì•½": 1.3,
            "í•˜ë£¨ì „": 1.2,
            "ì‹¤ì‹œê°„": 1.2,
            "ë‹¹ì¼": 1.2,
            "ê¸‰ì „": 1.2,
            "ê°€ê²©": 1.1,
            "ì…ì°°": 1.1,
            "ë°œì „ëŸ‰": 1.1,
            "ìˆ˜ìš”": 1.1
        }
    
    def calculate_keyword_score(self, text: str, query: str) -> float:
        """í‚¤ì›Œë“œ ê¸°ë°˜ ê´€ë ¨ì„± ì ìˆ˜ ê³„ì‚°"""
        score = 0.0
        text_lower = text.lower()
        query_lower = query.lower()
        
        # ì§ˆë¬¸ì˜ í‚¤ì›Œë“œë“¤
        query_words = query_lower.split()
        
        for word in query_words:
            if word in text_lower:
                # ì „ë ¥ì‹œì¥ íŠ¹í™” í‚¤ì›Œë“œë©´ ê°€ì¤‘ì¹˜ ì ìš©
                weight = self.power_market_keywords.get(word, 1.0)
                score += weight
        
        # ì „ì²´ ë‹¨ì–´ ìˆ˜ë¡œ ì •ê·œí™”
        if len(query_words) > 0:
            score = score / len(query_words)
        
        return score
    
    def semantic_search(self, query: str, top_k: Optional[int] = None) -> List[SearchResult]:
        """ì˜ë¯¸ì  ê²€ìƒ‰ (ì„ë² ë”© ê¸°ë°˜)"""
        try:
            if top_k is None:
                top_k = self.top_k
            
            # ì§ˆë¬¸ì„ ë²¡í„°ë¡œ ë³€í™˜
            query_embedding = self.text_embedder.encode_text(query)
            
            # ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ìœ ì‚¬ ë¬¸ì„œ ê²€ìƒ‰
            results = self.vector_db.search_similar(
                query_embedding=query_embedding,
                top_k=top_k * 2  # ë” ë§ì´ ê°€ì ¸ì™€ì„œ í›„ì²˜ë¦¬ë¡œ í•„í„°ë§
            )
            
            # ê²°ê³¼ ë³€í™˜ ë° í•„í„°ë§
            search_results = []
            for result in results:
                if result['similarity'] >= self.similarity_threshold:
                    search_result = SearchResult(
                        id=result['id'],
                        text=result['text'],
                        metadata=result['metadata'],
                        similarity=result['similarity'],
                        source_file=result['metadata'].get('source_file', ''),
                        relevance_score=result['similarity']
                    )
                    search_results.append(search_result)
            
            # ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
            search_results = search_results[:top_k]
            
            self.logger.info(f"ì˜ë¯¸ì  ê²€ìƒ‰ ì™„ë£Œ: {len(search_results)}ê°œ ê²°ê³¼")
            return search_results
            
        except Exception as e:
            self.logger.error(f"ì˜ë¯¸ì  ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def hybrid_search(self, query: str, 
                     semantic_weight: float = 0.7,
                     keyword_weight: float = 0.3,
                     top_k: Optional[int] = None) -> List[SearchResult]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ (ì˜ë¯¸ì  + í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°í•©)"""
        try:
            if top_k is None:
                top_k = self.top_k
            
            # ê°ê°ì˜ ê²€ìƒ‰ ì‹¤í–‰
            semantic_results = self.semantic_search(query, top_k * 2)
            keyword_results = self.keyword_search(query, top_k * 2)
            
            # ê²°ê³¼ í†µí•© (ID ê¸°ì¤€ìœ¼ë¡œ ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ í•©ì‚°)
            combined_results = {}
            
            # ì˜ë¯¸ì  ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€
            for result in semantic_results:
                combined_results[result.id] = result
                combined_results[result.id].relevance_score = (
                    result.similarity * semantic_weight
                )
            
            # í‚¤ì›Œë“œ ê²€ìƒ‰ ê²°ê³¼ ì¶”ê°€/ê°±ì‹ 
            for result in keyword_results:
                if result.id in combined_results:
                    # ê¸°ì¡´ ê²°ê³¼ì— í‚¤ì›Œë“œ ì ìˆ˜ ì¶”ê°€
                    combined_results[result.id].relevance_score += (
                        result.relevance_score * keyword_weight
                    )
                else:
                    # ìƒˆë¡œìš´ ê²°ê³¼ ì¶”ê°€
                    result.relevance_score = result.relevance_score * keyword_weight
                    combined_results[result.id] = result
            
            # ìµœì¢… ê²°ê³¼ ë¦¬ìŠ¤íŠ¸ ìƒì„± ë° ì •ë ¬
            final_results = list(combined_results.values())
            final_results.sort(key=lambda x: x.relevance_score, reverse=True)
            
            # ìƒìœ„ ê²°ê³¼ë§Œ ë°˜í™˜
            final_results = final_results[:top_k]
            
            self.logger.info(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì™„ë£Œ: {len(final_results)}ê°œ ê²°ê³¼")
            return final_results
            
        except Exception as e:
            self.logger.error(f"í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰ ì‹¤íŒ¨: {e}")
            return []
    
    def get_context_for_generation(self, search_results: List[SearchResult], 
                                 max_context_length: int = 4000) -> str:
        """ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë‹µë³€ ìƒì„±ìš© ì»¨í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        try:
            context_parts = []
            current_length = 0
            
            for i, result in enumerate(search_results):
                # ì†ŒìŠ¤ ì •ë³´ì™€ í•¨ê»˜ í…ìŠ¤íŠ¸ í¬ë§·íŒ…
                formatted_text = f"""
[ë¬¸ì„œ {i+1}] (ì¶œì²˜: {result.source_file}, ìœ ì‚¬ë„: {result.similarity:.3f})
{result.text}
"""
                
                # ê¸¸ì´ í™•ì¸
                if current_length + len(formatted_text) > max_context_length:
                    break
                
                context_parts.append(formatted_text)
                current_length += len(formatted_text)
            
            context = "\n".join(context_parts)
            
            self.logger.info(f"ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì™„ë£Œ: {len(context_parts)}ê°œ ë¬¸ì„œ, {len(context)}ì")
            return context
            
        except Exception as e:
            self.logger.error(f"ì»¨í…ìŠ¤íŠ¸ ìƒì„± ì‹¤íŒ¨: {e}")
            return ""

class PowerMarketRetriever(DocumentRetriever):
    """ì „ë ¥ì‹œì¥ íŠ¹í™” ê²€ìƒ‰ ì—”ì§„"""
    
    def __init__(self, vector_db, text_embedder, **kwargs):
        super().__init__(vector_db, text_embedder, **kwargs)
        
        # ì „ë ¥ì‹œì¥ ë„ë©”ì¸ë³„ ê°€ì¤‘ì¹˜
        self.domain_weights = {
            "ë°œì „ê³„íš": 1.5,
            "ê³„í†µìš´ì˜": 1.4,
            "ì „ë ¥ê±°ë˜": 1.3,
            "ì‹œì¥ìš´ì˜": 1.3,
            "ì˜ˆë¹„ë ¥": 1.2,
            "ì†¡ì „ì œì•½": 1.2
        }
    
    def smart_search(self, query: str, top_k: Optional[int] = None) -> List[SearchResult]:
        """ì „ë ¥ì‹œì¥ íŠ¹í™” ìŠ¤ë§ˆíŠ¸ ê²€ìƒ‰"""
        # ì§ˆë¬¸ì—ì„œ ë„ë©”ì¸ í‚¤ì›Œë“œ ê°ì§€
        detected_domains = []
        query_lower = query.lower()
        
        for domain, weight in self.domain_weights.items():
            if domain in query_lower:
                detected_domains.append((domain, weight))
        
        if detected_domains:
            # ë„ë©”ì¸ì´ ê°ì§€ë˜ë©´ í•´ë‹¹ ë„ë©”ì¸ì— íŠ¹í™”ëœ ê²€ìƒ‰
            self.logger.info(f"ê°ì§€ëœ ë„ë©”ì¸: {[d[0] for d in detected_domains]}")
            
            # ê°€ì¥ ë†’ì€ ê°€ì¤‘ì¹˜ì˜ ë„ë©”ì¸ìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ê²€ìƒ‰
            primary_domain = max(detected_domains, key=lambda x: x[1])[0]
            return self.search_by_category(query, primary_domain, top_k)
        else:
            # ì¼ë°˜ì ì¸ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
            return self.hybrid_search(query, top_k=top_k)
```

### 3.5 ë‹µë³€ ìƒì„± ëª¨ë“ˆ (generation/answer_generator.py)

```python
"""
ë‹µë³€ ìƒì„±(Generation) ëª¨ë“ˆ
- ê²€ìƒ‰ëœ ë¬¸ì„œë“¤ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„±
- ë‹¤ì–‘í•œ ì–¸ì–´ ëª¨ë¸ê³¼ ì—°ë™ ê°€ëŠ¥í•œ êµ¬ì¡°
"""

import logging
from typing import List, Dict, Optional
from dataclasses import dataclass
import json
import re

@dataclass
class GenerationResult:
    """ë‹µë³€ ìƒì„± ê²°ê³¼ë¥¼ ë‹´ëŠ” ë°ì´í„° í´ë˜ìŠ¤"""
    answer: str
    confidence: float
    sources: List[str]
    reasoning: str
    metadata: Dict

class AnswerGenerator:
    """ë‹µë³€ ìƒì„± ì—”ì§„ í´ë˜ìŠ¤"""
    
    def __init__(self, 
                 model_type: str = "rule_based",
                 temperature: float = 0.3,
                 max_length: int = 2000):
        """
        Args:
            model_type: ì‚¬ìš©í•  ëª¨ë¸ íƒ€ì… (rule_based, openai, claude ë“±)
            temperature: ìƒì„± ë‹¤ì–‘ì„± ì¡°ì ˆ (0.0-1.0)
            max_length: ìµœëŒ€ ë‹µë³€ ê¸¸ì´
        """
        self.logger = logging.getLogger(__name__)
        self.model_type = model_type
        self.temperature = temperature
        self.max_length = max_length
        
        # ì „ë ¥ì‹œì¥ íŠ¹í™” ë‹µë³€ í…œí”Œë¦¿
        self.answer_templates = {
            "ë°œì „ê³„íš": """
ë°œì „ê³„íšê³¼ ê´€ë ¨í•˜ì—¬ ë‹¤ìŒê³¼ ê°™ì´ ë‹µë³€ë“œë¦½ë‹ˆë‹¤:

{main_answer}

ê´€ë ¨ ê·œì •:
{regulations}

ì£¼ìš” ì ˆì°¨:
{procedures}

ì°¸ê³  ë¬¸ì„œ: {sources}
            """,
            
            "ê³„í†µìš´ì˜": """
ê³„í†µìš´ì˜ì— ëŒ€í•œ ë‹µë³€ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

{main_answer}

ìš´ì˜ ê¸°ì¤€:
{standards}

ì•ˆì „ ì¡°ì¹˜:
{safety_measures}

ì°¸ê³  ë¬¸ì„œ: {sources}
            """,
            
            "ì¼ë°˜": """
{main_answer}

ìƒì„¸ ë‚´ìš©:
{details}

ì°¸ê³  ë¬¸ì„œ: {sources}
            """
        }
    
    def extract_key_information(self, context: str, query: str) -> Dict[str, str]:
        """ì»¨í…ìŠ¤íŠ¸ì—ì„œ í•µì‹¬ ì •ë³´ ì¶”ì¶œ"""
        try:
            info = {
                "main_points": [],
                "regulations": [],
                "procedures": [],
                "standards": [],
                "safety_measures": []
            }
            
            # í…ìŠ¤íŠ¸ë¥¼ ë¬¸ì¥ ë‹¨ìœ„ë¡œ ë¶„í• 
            sentences = re.split(r'[.!?]\s+', context)
            
            for sentence in sentences:
                sentence = sentence.strip()
                if not sentence:
                    continue
                
                # ê·œì • ê´€ë ¨ ë¬¸ì¥
                if any(keyword in sentence for keyword in ["ì¡°", "í•­", "ê·œì •", "ê·œì¹™", "ê¸°ì¤€"]):
                    info["regulations"].append(sentence)
                
                # ì ˆì°¨ ê´€ë ¨ ë¬¸ì¥
                elif any(keyword in sentence for keyword in ["ì ˆì°¨", "ë‹¨ê³„", "ê³¼ì •", "ìˆœì„œ"]):
                    info["procedures"].append(sentence)
                
                # ê¸°ì¤€ ê´€ë ¨ ë¬¸ì¥
                elif any(keyword in sentence for keyword in ["ê¸°ì¤€", "í‘œì¤€", "ìš”êµ¬ì‚¬í•­"]):
                    info["standards"].append(sentence)
                
                # ì•ˆì „ ê´€ë ¨ ë¬¸ì¥
                elif any(keyword in sentence for keyword in ["ì•ˆì „", "ë³´ì•ˆ", "ìœ„í—˜", "ì£¼ì˜"]):
                    info["safety_measures"].append(sentence)
                
                # ì¼ë°˜ì ì¸ í•µì‹¬ ë‚´ìš©
                else:
                    info["main_points"].append(sentence)
            
            return info
            
        except Exception as e:
            self.logger.error(f"í•µì‹¬ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return {"main_points": [context], "regulations": [], "procedures": [], 
                   "standards": [], "safety_measures": []}
    
    def determine_domain(self, query: str, context: str) -> str:
        """ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¥¼ ë°”íƒ•ìœ¼ë¡œ ë„ë©”ì¸ íŒë‹¨"""
        domain_keywords = {
            "ë°œì „ê³„íš": ["ë°œì „ê³„íš", "í•˜ë£¨ì „", "ë‹¹ì¼", "ì‹¤ì‹œê°„", "ê³„íšìˆ˜ë¦½"],
            "ê³„í†µìš´ì˜": ["ê³„í†µìš´ì˜", "ìš´ì˜ê¸°ì¤€", "ì•ˆì „ìš´ì „", "ê³„í†µì œì•½"],
            "ì „ë ¥ê±°ë˜": ["ì „ë ¥ê±°ë˜", "ì…ì°°", "ê°€ê²©", "ì‹œì¥"],
            "ì˜ˆë¹„ë ¥": ["ì˜ˆë¹„ë ¥", "ì˜ˆë¹„ë ¥ì‹œì¥", "ì˜ˆë¹„ë ¥ìš©ëŸ‰"],
            "ì†¡ì „ì œì•½": ["ì†¡ì „ì œì•½", "ì œì•½ì •ë³´", "ê³„í†µì œì•½"]
        }
        
        combined_text = (query + " " + context).lower()
        
        domain_scores = {}
        for domain, keywords in domain_keywords.items():
            score = sum(1 for keyword in keywords if keyword in combined_text)
            domain_scores[domain] = score
        
        # ê°€ì¥ ë†’ì€ ì ìˆ˜ì˜ ë„ë©”ì¸ ë°˜í™˜
        if domain_scores:
            best_domain = max(domain_scores, key=domain_scores.get)
            if domain_scores[best_domain] > 0:
                return best_domain
        
        return "ì¼ë°˜"
    
    def generate_rule_based_answer(self, context: str, query: str, sources: List[str]) -> GenerationResult:
        """ê·œì¹™ ê¸°ë°˜ ë‹µë³€ ìƒì„±"""
        try:
            # ë„ë©”ì¸ íŒë‹¨
            domain = self.determine_domain(query, context)
            
            # í•µì‹¬ ì •ë³´ ì¶”ì¶œ
            key_info = self.extract_key_information(context, query)
            
            # ë©”ì¸ ë‹µë³€ ìƒì„±
            main_answer = self._generate_main_answer(query, key_info["main_points"])
            
            # í…œí”Œë¦¿ ì„ íƒ ë° ë‹µë³€ êµ¬ì„±
            template = self.answer_templates.get(domain, self.answer_templates["ì¼ë°˜"])
            
            if domain == "ë°œì „ê³„íš":
                answer = template.format(
                    main_answer=main_answer,
                    regulations=self._format_list(key_info["regulations"]),
                    procedures=self._format_list(key_info["procedures"]),
                    sources=", ".join(sources)
                )
            elif domain == "ê³„í†µìš´ì˜":
                answer = template.format(
                    main_answer=main_answer,
                    standards=self._format_list(key_info["standards"]),
                    safety_measures=self._format_list(key_info["safety_measures"]),
                    sources=", ".join(sources)
                )
            else:
                answer = template.format(
                    main_answer=main_answer,
                    details=self._format_list(key_info["regulations"] + key_info["procedures"]),
                    sources=", ".join(sources)
                )
            
            # ì‹ ë¢°ë„ ê³„ì‚°
            confidence = self.calculate_confidence(context, query, answer)
            
            # ì¶”ë¡  ê³¼ì • ì„¤ëª…
            reasoning = f"ë„ë©”ì¸: {domain}, ì°¸ì¡° ë¬¸ì„œ ìˆ˜: {len(sources)}, í•µì‹¬ ì •ë³´: {len(key_info['main_points'])}ê°œ í¬ì¸íŠ¸"
            
            result = GenerationResult(
                answer=answer.strip(),
                confidence=confidence,
                sources=sources,
                reasoning=reasoning,
                metadata={
                    "domain": domain,
                    "generation_method": "rule_based",
                    "context_length": len(context),
                    "query_length": len(query)
                }
            )
            
            self.logger.info(f"ê·œì¹™ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì™„ë£Œ (ì‹ ë¢°ë„: {confidence:.3f})")
            return result
            
        except Exception as e:
            self.logger.error(f"ê·œì¹™ ê¸°ë°˜ ë‹µë³€ ìƒì„± ì‹¤íŒ¨: {e}")
            return GenerationResult(
                answer="ì£„ì†¡í•©ë‹ˆë‹¤. ë‹µë³€ ìƒì„± ì¤‘ ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤.",
                confidence=0.0,
                sources=sources,
                reasoning="ë‹µë³€ ìƒì„± ì‹¤íŒ¨",
                metadata={"error": str(e)}
            )
    
    def _generate_main_answer(self, query: str, main_points: List[str]) -> str:
        """ë©”ì¸ ë‹µë³€ ìƒì„±"""
        if not main_points:
            return "ê´€ë ¨ ì •ë³´ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."
        
        # ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ í¬ì¸íŠ¸ë“¤ ì„ íƒ (ìµœëŒ€ 3ê°œ)
        relevant_points = main_points[:3]
        
        # ì§ˆë¬¸ ìœ í˜•ì— ë”°ë¥¸ ë‹µë³€ ì‹œì‘
        if "ë¬´ì—‡" in query or "ë­" in query:
            answer_start = "ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:"
        elif "ì–´ë–»ê²Œ" in query or "ë°©ë²•" in query:
            answer_start = "ë‹¤ìŒê³¼ ê°™ì€ ë°©ë²•ìœ¼ë¡œ ìˆ˜í–‰ë©ë‹ˆë‹¤:"
        elif "ì–¸ì œ" in query or "ì‹œê°„" in query:
            answer_start = "ë‹¤ìŒê³¼ ê°™ì€ ì‹œì ì— ì‹¤í–‰ë©ë‹ˆë‹¤:"
        elif "ì™œ" in query or "ì´ìœ " in query:
            answer_start = "ë‹¤ìŒê³¼ ê°™ì€ ì´ìœ  ë•Œë¬¸ì…ë‹ˆë‹¤:"
        else:
            answer_start = "ê´€ë ¨ ë‚´ìš©ì€ ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:"
        
        return answer_start + "\n\n" + "\n\n".join(f"â€¢ {point}" for point in relevant_points)
    
    def _format_list(self, items: List[str]) -> str:
        """ë¦¬ìŠ¤íŠ¸ë¥¼ ì½ê¸° ì¢‹ì€ í˜•íƒœë¡œ í¬ë§·íŒ…"""
        if not items:
            return "í•´ë‹¹ ì •ë³´ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        formatted_items = []
        for i, item in enumerate(items[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ
            formatted_items.append(f"{i}. {item}")
        
        return "\n".join(formatted_items)
    
    def generate_answer(self, context: str, query: str, sources: List[str]) -> GenerationResult:
        """ì§ˆë¬¸ì— ëŒ€í•œ ë‹µë³€ ìƒì„± (ë©”ì¸ ì¸í„°í˜ì´ìŠ¤)"""
        try:
            self.logger.info(f"ë‹µë³€ ìƒì„± ì‹œì‘ - ëª¨ë¸: {self.model_type}")
            
            if self.model_type == "rule_based":
                return self.generate_rule_based_answer(context, query, sources)
            else:
                # ë‹¤ë¥¸ ëª¨ë¸ íƒ€ì…ë“¤ì€ ì¶”í›„ êµ¬í˜„
                self.logger.warning(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ëª¨ë¸ íƒ€ì…: {self.model_type}")
                return self.generate_rule_based_answer(context, query, sources)
                
        except Exception as e:
            self.logger.error(f"ë‹µë³€ ìƒì„± ì „ì²´ ì‹¤íŒ¨: {e}")
            return GenerationResult(
                answer="ì‹œìŠ¤í…œ ì˜¤ë¥˜ë¡œ ì¸í•´ ë‹µë³€ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.",
                confidence=0.0,
                sources=sources,
                reasoning="ì‹œìŠ¤í…œ ì˜¤ë¥˜",
                metadata={"error": str(e)}
            )

class PowerMarketAnswerGenerator(AnswerGenerator):
    """ì „ë ¥ì‹œì¥ íŠ¹í™” ë‹µë³€ ìƒì„±ê¸°"""
    
    def __init__(self, **kwargs):
        super().__init__(**kwargs)
        
        # ì „ë ¥ì‹œì¥ íŠ¹í™” ë‹µë³€ íŒ¨í„´
        self.specialized_patterns = {
            "regulation_pattern": r"ì œ\s*(\d+(?:\.\d+)*)\s*ì¡°",
            "article_pattern": r"(\d+(?:\.\d+)*)\s*í•­",
            "schedule_pattern": r"ë³„í‘œ\s*(\d+)",
            "time_pattern": r"(\d+)ì‹œ\s*(\d+)ë¶„",
            "percentage_pattern": r"(\d+(?:\.\d+)*)\s*%"
        }
    
    def enhance_answer_with_regulations(self, answer: str, context: str) -> str:
        """ë‹µë³€ì— ê·œì • ì •ë³´ ê°•í™”"""
        # ê·œì • ë²ˆí˜¸ ì¶”ì¶œ ë° ê°•ì¡°
        regulations = re.findall(self.specialized_patterns["regulation_pattern"], context)
        
        if regulations:
            reg_info = f"\n\nğŸ“‹ ê´€ë ¨ ê·œì •: " + ", ".join([f"ì œ{reg}ì¡°" for reg in regulations[:3]])
            answer += reg_info
        
        return answer
```

---

## 4. ì„¤ì • ë° ìŠ¤í¬ë¦½íŠ¸

### 4.1 ì‹œìŠ¤í…œ ì„¤ì • (config/config.yaml)

```yaml
# RAG ì‹œìŠ¤í…œ ì„¤ì • íŒŒì¼

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤ ì„¤ì •
VECTOR_DB_TYPE: "chromadb"  # chromadb ë˜ëŠ” faiss
VECTOR_DB_PATH: "./vector_db"
COLLECTION_NAME: "power_market_docs"

# ì„ë² ë”© ëª¨ë¸ ì„¤ì •
EMBEDDING_MODEL: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
EMBEDDING_DIMENSION: 384

# ë¬¸ì„œ ì²˜ë¦¬ ì„¤ì •
CHUNK_SIZE: 1000  # í…ìŠ¤íŠ¸ë¥¼ ë‚˜ëˆ„ëŠ” í¬ê¸°
CHUNK_OVERLAP: 200  # ê²¹ì¹˜ëŠ” ë¶€ë¶„ í¬ê¸°
MAX_TOKENS: 4000  # ìµœëŒ€ í† í° ìˆ˜

# ê²€ìƒ‰ ì„¤ì •
TOP_K: 5  # ìƒìœ„ ëª‡ ê°œ ë¬¸ì„œë¥¼ ê°€ì ¸ì˜¬ì§€
SIMILARITY_THRESHOLD: 0.7  # ìœ ì‚¬ë„ ì„ê³„ê°’

# API ì„¤ì •
API_HOST: "0.0.0.0"
API_PORT: 8000
DEBUG: true

# ë¡œê·¸ ì„¤ì •
LOG_LEVEL: "INFO"
LOG_FILE: "./logs/rag_system.log"

# ì „ë ¥ì‹œì¥ íŠ¹í™” ì„¤ì •
POWER_MARKET_DOMAINS:
  - "ë°œì „ê³„íš"
  - "ê³„í†µìš´ì˜"
  - "ì „ë ¥ê±°ë˜"
  - "ì‹œì¥ìš´ì˜"
  - "ì˜ˆë¹„ë ¥"
  - "ì†¡ì „ì œì•½"
```

### 4.2 íŒ¨í‚¤ì§€ ìš”êµ¬ì‚¬í•­ (requirements.txt)

```
# RAG ì‹œìŠ¤í…œì„ ìœ„í•œ í•„ìˆ˜ ë¼ì´ë¸ŒëŸ¬ë¦¬ë“¤

# ë²¡í„° ë°ì´í„°ë² ì´ìŠ¤
chromadb==0.4.22
faiss-cpu==1.7.4

# í…ìŠ¤íŠ¸ ì„ë² ë”© (ë²¡í„° ë³€í™˜)
sentence-transformers==2.2.2
transformers==4.36.0
torch==2.0.1

# ë¬¸ì„œ ì²˜ë¦¬
langchain==0.1.0
PyPDF2==3.0.1
python-docx==0.8.11
openpyxl==3.1.2

# ì›¹ API ì„œë²„
fastapi==0.104.1
uvicorn==0.24.0
pydantic==2.5.0

# ë°ì´í„° ì²˜ë¦¬
pandas==2.1.4
numpy==1.24.3

# ì„¤ì • ê´€ë¦¬
python-dotenv==1.0.0
pyyaml==6.0.1

# ë¡œê¹…
loguru==0.7.2

# ê°œë°œìš© ë„êµ¬
jupyter==1.0.0
pytest==7.4.3
```

### 4.3 ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ (install.sh)

```bash
#!/bin/bash

# ì „ë ¥ì‹œì¥ RAG ì‹œìŠ¤í…œ ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸
# Ubuntu/WSL í™˜ê²½ì—ì„œ ì‹¤í–‰

echo "ğŸš€ ì „ë ¥ì‹œì¥ RAG ì‹œìŠ¤í…œ ì„¤ì¹˜ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤..."

# 1. Python ë²„ì „ í™•ì¸
echo "1. Python ë²„ì „ í™•ì¸ ì¤‘..."
python3 --version

if [ $? -ne 0 ]; then
    echo "âŒ Python3ê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤. Python3ë¥¼ ë¨¼ì € ì„¤ì¹˜í•´ì£¼ì„¸ìš”."
    exit 1
fi

# 2. ê°€ìƒí™˜ê²½ ìƒì„±
echo "2. Python ê°€ìƒí™˜ê²½ ìƒì„± ì¤‘..."
python3 -m venv venv

if [ $? -ne 0 ]; then
    echo "âŒ ê°€ìƒí™˜ê²½ ìƒì„± ì‹¤íŒ¨. python3-venvë¥¼ ì„¤ì¹˜í•´ì£¼ì„¸ìš”:"
    echo "sudo apt update && sudo apt install python3-venv"
    exit 1
fi

# 3. ê°€ìƒí™˜ê²½ í™œì„±í™”
echo "3. ê°€ìƒí™˜ê²½ í™œì„±í™” ì¤‘..."
source venv/bin/activate

# 4. pip ì—…ê·¸ë ˆì´ë“œ
echo "4. pip ì—…ê·¸ë ˆì´ë“œ ì¤‘..."
pip install --upgrade pip

# 5. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
echo "5. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì¤‘..."
echo "â³ ì´ ê³¼ì •ì€ ëª‡ ë¶„ ì •ë„ ì†Œìš”ë  ìˆ˜ ìˆìŠµë‹ˆë‹¤..."

pip install -r requirements.txt

if [ $? -ne 0 ]; then
    echo "âŒ íŒ¨í‚¤ì§€ ì„¤ì¹˜ ì‹¤íŒ¨. ë„¤íŠ¸ì›Œí¬ ì—°ê²°ì„ í™•ì¸í•´ì£¼ì„¸ìš”."
    exit 1
fi

# 6. í•„ìš”í•œ ë””ë ‰í† ë¦¬ ìƒì„±
echo "6. í•„ìš”í•œ ë””ë ‰í† ë¦¬ í™•ì¸ ì¤‘..."
mkdir -p logs
mkdir -p documents

# 7. ì„¤ì¹˜ ì™„ë£Œ ë©”ì‹œì§€
echo ""
echo "âœ… ì „ë ¥ì‹œì¥ RAG ì‹œìŠ¤í…œ ì„¤ì¹˜ê°€ ì™„ë£Œë˜ì—ˆìŠµë‹ˆë‹¤!"
echo ""
echo "ğŸ“ ë‹¤ìŒ ëª…ë ¹ì–´ë¡œ ì‹œìŠ¤í…œì„ ì‹¤í–‰í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤:"
echo ""
echo "   # ê°€ìƒí™˜ê²½ í™œì„±í™”"
echo "   source venv/bin/activate"
echo ""
echo "   # ë©”ì¸ ì‹œìŠ¤í…œ ì‹¤í–‰"
echo "   python power_market_rag.py"
echo ""
echo "   # API ì„œë²„ ì‹¤í–‰"
echo "   python api/api_server.py"
echo ""
echo "ğŸ“š ë¬¸ì„œë¥¼ documents/ í´ë”ì— ë„£ê³  ì‹œìŠ¤í…œì„ ì‹¤í–‰í•˜ì„¸ìš”."
echo ""
echo "ğŸŒ API ì„œë²„ë¥¼ ì‹¤í–‰í•˜ë©´ http://localhost:8000 ì—ì„œ ì›¹ ì¸í„°í˜ì´ìŠ¤ë¥¼ ì‚¬ìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤."
```

### 4.4 ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ (run.sh)

```bash
#!/bin/bash

# ì „ë ¥ì‹œì¥ RAG ì‹œìŠ¤í…œ ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸

# ìƒ‰ìƒ ì •ì˜
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[1;33m'
BLUE='\033[0;34m'
NC='\033[0m' # No Color

# í•¨ìˆ˜: ì œëª© ì¶œë ¥
print_title() {
    echo -e "${BLUE}"
    echo "=================================================="
    echo "         âš¡ ì „ë ¥ì‹œì¥ RAG ì‹œìŠ¤í…œ ì‹¤í–‰ê¸° âš¡"
    echo "=================================================="
    echo -e "${NC}"
}

# í•¨ìˆ˜: ê°€ìƒí™˜ê²½ í™•ì¸ ë° í™œì„±í™”
activate_venv() {
    if [ -d "venv" ]; then
        echo -e "${GREEN}ğŸ“¦ ê°€ìƒí™˜ê²½ í™œì„±í™” ì¤‘...${NC}"
        source venv/bin/activate
        echo -e "${GREEN}âœ… ê°€ìƒí™˜ê²½ì´ í™œì„±í™”ë˜ì—ˆìŠµë‹ˆë‹¤.${NC}"
    else
        echo -e "${RED}âŒ ê°€ìƒí™˜ê²½ì´ ì—†ìŠµë‹ˆë‹¤. ë¨¼ì € install.shë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.${NC}"
        exit 1
    fi
}

# í•¨ìˆ˜: API ì„œë²„ ì‹¤í–‰
run_api_server() {
    echo -e "${YELLOW}ğŸŒ API ì„œë²„ë¥¼ ì‹œì‘í•©ë‹ˆë‹¤...${NC}"
    echo -e "${BLUE}ğŸ“ ì›¹ ì¸í„°í˜ì´ìŠ¤: http://localhost:8000${NC}"
    echo -e "${BLUE}ğŸ“– API ë¬¸ì„œ: http://localhost:8000/docs${NC}"
    echo ""
    echo -e "${YELLOW}âš ï¸  ì„œë²„ë¥¼ ì¤‘ì§€í•˜ë ¤ë©´ Ctrl+Cë¥¼ ëˆ„ë¥´ì„¸ìš”.${NC}"
    echo ""
    
    cd api
    python api_server.py
}

# í•¨ìˆ˜: ë©”ì¸ ì‹œìŠ¤í…œ ì‹¤í–‰
run_main_system() {
    echo -e "${YELLOW}ğŸ”„ ë©”ì¸ RAG ì‹œìŠ¤í…œì„ ì‹œì‘í•©ë‹ˆë‹¤...${NC}"
    python power_market_rag.py
}

# ë©”ì¸ í•¨ìˆ˜
main() {
    print_title
    
    # í˜„ì¬ ë””ë ‰í† ë¦¬ í™•ì¸
    if [ ! -f "power_market_rag.py" ]; then
        echo -e "${RED}âŒ power_market_rag.py íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.${NC}"
        echo -e "${RED}   ì˜¬ë°”ë¥¸ í”„ë¡œì íŠ¸ ë””ë ‰í† ë¦¬ì—ì„œ ì‹¤í–‰í•´ì£¼ì„¸ìš”.${NC}"
        exit 1
    fi
    
    # ê°€ìƒí™˜ê²½ í™œì„±í™”
    activate_venv
    
    # ì‹¤í–‰ ëª¨ë“œ ì„ íƒ
    echo -e "${BLUE}ì‹¤í–‰í•  ëª¨ë“œë¥¼ ì„ íƒí•˜ì„¸ìš”:${NC}"
    echo "1) ğŸŒ API ì„œë²„ ì‹¤í–‰ (ì›¹ ì¸í„°í˜ì´ìŠ¤ í¬í•¨)"
    echo "2) ğŸ”„ ë©”ì¸ ì‹œìŠ¤í…œ ì‹¤í–‰ (ì½˜ì†” ëª¨ë“œ)"
    echo "3) ğŸ§ª ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"
    echo "4) ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸"
    echo "5) ğŸšª ì¢…ë£Œ"
    echo ""
    
    read -p "ì„ íƒ (1-5): " choice
    
    case $choice in
        1)
            run_api_server
            ;;
        2)
            run_main_system
            ;;
        3)
            run_tests
            ;;
        4)
            echo -e "${YELLOW}ğŸ“Š ì‹œìŠ¤í…œ ìƒíƒœë¥¼ í™•ì¸í•©ë‹ˆë‹¤...${NC}"
            python -c "
from power_market_rag import PowerMarketRAG
rag = PowerMarketRAG()
status = rag.get_system_status()
print(f'ì‹œìŠ¤í…œ ìƒíƒœ: {status}')
            "
            ;;
        5)
            echo -e "${GREEN}ğŸ‘‹ ì‹œìŠ¤í…œì„ ì¢…ë£Œí•©ë‹ˆë‹¤.${NC}"
            exit 0
            ;;
        *)
            echo -e "${RED}âŒ ì˜ëª»ëœ ì„ íƒì…ë‹ˆë‹¤. 1-5 ì¤‘ì—ì„œ ì„ íƒí•´ì£¼ì„¸ìš”.${NC}"
            exit 1
            ;;
    esac
}

# ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
main "$@"
```

---

## 5. ì˜ˆì‹œ ë¬¸ì„œ

### 5.1 ë°œì „ê³„íš ê°€ì´ë“œ (documents/ë°œì „ê³„íš_ê°€ì´ë“œ.md)

```markdown
# ì „ë ¥ì‹œì¥ ë°œì „ê³„íš ìˆ˜ë¦½ ê°€ì´ë“œ

## 1. í•˜ë£¨ì „ë°œì „ê³„íš

### 1.1 ê°œìš”
í•˜ë£¨ì „ë°œì „ê³„íšì€ ì „ë ¥ê±°ë˜ì¼ ì „ì¼ì— ìˆ˜ë¦½ë˜ëŠ” ë°œì „ê³„íšìœ¼ë¡œ, ì „ë ¥ì‹œì¥ìš´ì˜ê·œì¹™ ì œ16.4.1ì¡°ì— ë”°ë¼ ìˆ˜ë¦½ë©ë‹ˆë‹¤.

### 1.2 ìˆ˜ë¦½ ì ˆì°¨
í•˜ë£¨ì „ë°œì „ê³„íš ìˆ˜ë¦½ ì ˆì°¨ëŠ” ë‹¤ìŒê³¼ ê°™ìŠµë‹ˆë‹¤:

1. **11ì‹œ**: ì´ˆê¸°ì…ì°° ì…ë ¥
2. **16ì‹œ**: ì œì£¼ìˆ˜ìš”ì˜ˆì¸¡ ì—°ê³„
3. **17ì‹œ**: í•˜ë£¨ì „ë°œì „ê³„íš ìˆ˜ë¦½ ë° í•˜ë£¨ì „ì—ë„ˆì§€ê°€ê²© ì‚°ì •
4. **18ì‹œ**: í•˜ë£¨ì „ë°œì „ê³„íš ë° í•˜ë£¨ì „ì—ë„ˆì§€ê°€ê²© ê³µí‘œ

### 1.3 ëª©ì í•¨ìˆ˜
ë°œì „ê³„íšìˆ˜ë¦½ê¸°ê°„ ë™ì•ˆì˜ ì´ ë°œì „ë¹„ìš© ë° ìˆ˜ìš”ê°ì¶•ë¹„ìš© ìµœì†Œí™”ë¥¼ ëª©ì ìœ¼ë¡œ ë‹¤ìŒ ì œì•½ì„ ê³ ë ¤í•©ë‹ˆë‹¤:
- ìš´ì˜ì˜ˆë¹„ë ¥ ì œì•½
- ë°œì „ê¸° ìê¸°ì œì•½
- ì†¡ì „ì œì•½

## 2. ë‹¹ì¼ë°œì „ê³„íš

### 2.1 ê°œìš”
ë‹¹ì¼ë°œì „ê³„íšì€ ì „ë ¥ì‹œì¥ìš´ì˜ê·œì¹™ ì œ16.4.3ì¡°ì— ë”°ë¼ ë§¤ì‹œê°„ë§ˆë‹¤ í†µì§€ë˜ëŠ” ë°œì „ê³„íšì…ë‹ˆë‹¤.

### 2.2 ìˆ˜ë¦½ íŠ¹ì§•
- ë°œì „ê³„íš ìˆ˜ë¦½ì‹œ ìµœì‹  ì˜ˆì¸¡ì •ë³´ ì‚¬ìš©
- ìµœì‹  ì†¡ì „ì œì•½ ë°˜ì˜
- ì‹¤ì‹œê°„ ìˆ˜ìš”ë³€í™” ëŒ€ì‘

## 3. ì‹¤ì‹œê°„ë°œì „ê³„íš

### 3.1 ê°œìš”
ì‹¤ì‹œê°„ë°œì „ê³„íšì€ ì‹¤ì‹œê°„ì‹œì¥ ë‹¨ìœ„ê±°ë˜ì‹œê°„ì¸ 15ë¶„ë§ˆë‹¤ í†µì§€ë˜ëŠ” ë°œì „ê³„íšì…ë‹ˆë‹¤.

### 3.2 íŠ¹ì§•
- 15ë¶„ ë‹¨ìœ„ ìš´ì˜
- ìµœì‹ ì˜ˆì¸¡ì •ë³´ ë°˜ì˜
- ì†¡ì „ì œì•½ ì‹¤ì‹œê°„ ì ìš©
```

### 5.2 ê³„í†µìš´ì˜ê¸°ì¤€ (documents/ê³„í†µìš´ì˜ê¸°ì¤€.md)

```markdown
# ì „ë ¥ê³„í†µ ìš´ì˜ê¸°ì¤€

## 1. ê³„í†µìš´ì˜ì˜ ê¸°ë³¸ì›ì¹™

### 1.1 ì•ˆì „ì„± ìš°ì„ 
ì „ë ¥ê³„í†µ ìš´ì˜ì—ì„œ ê°€ì¥ ì¤‘ìš”í•œ ê²ƒì€ ì•ˆì „ì„± í™•ë³´ì…ë‹ˆë‹¤.
- ê³„í†µ ì•ˆì •ë„ ìœ ì§€
- ì„¤ë¹„ ë³´í˜¸
- ì¸ëª… ì•ˆì „ í™•ë³´

### 1.2 ì‹ ë¢°ì„± í™•ë³´
- ì „ë ¥ ê³µê¸‰ì˜ ì—°ì†ì„± ë³´ì¥
- ì •ì „ ìµœì†Œí™”
- ë¹ ë¥¸ ë³µêµ¬ ëŠ¥ë ¥ í™•ë³´

## 2. ê³„í†µìš´ì˜ ê¸°ì¤€

### 2.1 ì£¼íŒŒìˆ˜ ê´€ë¦¬
í•œêµ­ ì „ë ¥ê³„í†µì˜ ì •ê²© ì£¼íŒŒìˆ˜ëŠ” 60Hzì…ë‹ˆë‹¤.

#### 2.1.1 ì£¼íŒŒìˆ˜ í—ˆìš©ë²”ìœ„
- **ì •ìƒìš´ì „**: 59.8 ~ 60.2Hz (Â±0.2Hz)
- **ê²½ê³„ìš´ì „**: 59.5 ~ 60.5Hz (Â±0.5Hz)
- **ë¹„ìƒìš´ì „**: 59.0 ~ 61.0Hz (Â±1.0Hz)

#### 2.1.2 ì£¼íŒŒìˆ˜ ì œì–´ ë°©ë²•
1. **1ì°¨ ì œì–´**: ì¡°ì†ê¸°ì— ì˜í•œ ìë™ ì œì–´ (ìˆ˜ì´ˆ ì´ë‚´)
2. **2ì°¨ ì œì–´**: AGC(Automatic Generation Control)ì— ì˜í•œ ì œì–´ (ìˆ˜ë¶„ ì´ë‚´)
3. **3ì°¨ ì œì–´**: ìš´ì „ì›ì— ì˜í•œ ìˆ˜ë™ ì œì–´ (ìˆ˜ì‹­ë¶„ ì´ë‚´)

### 2.2 ì˜ˆë¹„ë ¥ ê´€ë¦¬

#### 2.2.1 ì˜ˆë¹„ë ¥ ì¢…ë¥˜
1. **ìš´ì „ì˜ˆë¹„ë ¥**: í˜„ì¬ ìš´ì „ ì¤‘ì¸ ë°œì „ê¸°ì˜ ì—¬ìœ  ìš©ëŸ‰
2. **ì •ì§€ì˜ˆë¹„ë ¥**: ë¹ ë¥¸ ì‹œê°„ ë‚´ ê¸°ë™ ê°€ëŠ¥í•œ ì •ì§€ ë°œì „ê¸° ìš©ëŸ‰
3. **êµì²´ì˜ˆë¹„ë ¥**: ì¥ì‹œê°„ ìš´ì „ ê°€ëŠ¥í•œ ì˜ˆë¹„ ë°œì „ê¸° ìš©ëŸ‰

#### 2.2.2 ì˜ˆë¹„ë ¥ ìš”êµ¬ëŸ‰
- **ìš´ì „ì˜ˆë¹„ë ¥**: ìµœëŒ€ë¶€í•˜ì˜ 7% ì´ìƒ
- **ìˆœë™ì˜ˆë¹„ë ¥**: ìµœëŒ€ ë°œì „ê¸° ìš©ëŸ‰ ì´ìƒ
- **ê¸°ë™ì˜ˆë¹„ë ¥**: ìµœëŒ€ë¶€í•˜ì˜ 10% ì´ìƒ
```

---

## 6. ì„¤ì¹˜ ë° ì‹¤í–‰

### 6.1 ì‹œìŠ¤í…œ ìš”êµ¬ì‚¬í•­

#### ìµœì†Œ ìš”êµ¬ì‚¬í•­
- **OS**: Ubuntu 20.04+ / WSL2 / macOS 10.15+
- **Python**: 3.8 ì´ìƒ
- **RAM**: 4GB ì´ìƒ
- **Storage**: 2GB ì´ìƒ ì—¬ìœ  ê³µê°„

#### ê¶Œì¥ ì‚¬ì–‘
- **OS**: Ubuntu 22.04 / WSL2
- **Python**: 3.10 ì´ìƒ
- **RAM**: 8GB ì´ìƒ
- **Storage**: 10GB ì´ìƒ ì—¬ìœ  ê³µê°„

### 6.2 ì„¤ì¹˜ ë°©ë²•

#### ìë™ ì„¤ì¹˜ (ê¶Œì¥)

```bash
# 1. í”„ë¡œì íŠ¸ ë‹¤ìš´ë¡œë“œ
git clone <repository-url>
cd power_market_rag

# 2. ì„¤ì¹˜ ìŠ¤í¬ë¦½íŠ¸ ì‹¤í–‰
chmod +x install.sh
./install.sh
```

#### ìˆ˜ë™ ì„¤ì¹˜

```bash
# 1. Python ê°€ìƒí™˜ê²½ ìƒì„±
python3 -m venv venv
source venv/bin/activate

# 2. í•„ìš”í•œ íŒ¨í‚¤ì§€ ì„¤ì¹˜
pip install --upgrade pip
pip install -r requirements.txt

# 3. ë””ë ‰í† ë¦¬ ìƒì„±
mkdir -p logs documents
```

### 6.3 ì‹¤í–‰ ë°©ë²•

#### ê°„í¸ ì‹¤í–‰

```bash
# ì‹¤í–‰ ìŠ¤í¬ë¦½íŠ¸ ì‚¬ìš©
chmod +x run.sh
./run.sh
```

#### ê°œë³„ ì‹¤í–‰

```bash
# ê°€ìƒí™˜ê²½ í™œì„±í™”
source venv/bin/activate

# ë°ëª¨ ì‹¤í–‰
python demo.py

# API ì„œë²„ ì‹¤í–‰
python api/api_server.py

# ë©”ì¸ ì‹œìŠ¤í…œ ì‹¤í–‰
python power_market_rag.py
```

---

## 7. ì‚¬ìš©ë²• ê°€ì´ë“œ

### 7.1 ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‚¬ìš©

1. **API ì„œë²„ ì‹œì‘**
   ```bash
   python api/api_server.py
   ```

2. **ì›¹ ë¸Œë¼ìš°ì € ì ‘ì†**
   - URL: `http://localhost:8000`

3. **ì§ˆë¬¸ ì…ë ¥**
   - ì§ˆë¬¸ ì…ë ¥ì°½ì— ì „ë ¥ì‹œì¥ ê´€ë ¨ ì§ˆë¬¸ ì‘ì„±
   - ê²€ìƒ‰ ë°©ë²• ì„ íƒ (í•˜ì´ë¸Œë¦¬ë“œ ê¶Œì¥)
   - "ì§ˆë¬¸í•˜ê¸°" ë²„íŠ¼ í´ë¦­

4. **ë‹µë³€ í™•ì¸**
   - ìƒì„±ëœ ë‹µë³€ê³¼ ì‹ ë¢°ë„ í™•ì¸
   - ì°¸ê³  ë¬¸ì„œ ì¶œì²˜ í™•ì¸

### 7.2 ë¬¸ì„œ ì¶”ê°€ ë°©ë²•

#### ë°©ë²• 1: íŒŒì¼ ì§ì ‘ ë³µì‚¬
```bash
cp your_document.pdf documents/
```

#### ë°©ë²• 2: ì›¹ ì¸í„°í˜ì´ìŠ¤ ì—…ë¡œë“œ
1. ì›¹ ì¸í„°í˜ì´ìŠ¤ ì ‘ì†
2. íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ ì´ìš©
3. ë“œë˜ê·¸&ë“œë¡­ ë˜ëŠ” íŒŒì¼ ì„ íƒ

### 7.3 ì˜ˆì‹œ ì§ˆë¬¸ë“¤

#### ë°œì „ê³„íš ê´€ë ¨
- "í•˜ë£¨ì „ë°œì „ê³„íšì€ ì–¸ì œ ìˆ˜ë¦½ë˜ë‚˜ìš”?"
- "ì‹¤ì‹œê°„ë°œì „ê³„íšì˜ ìš´ì˜ ì£¼ê¸°ëŠ”?"
- "ë°œì „ê³„íš ìˆ˜ë¦½ ì ˆì°¨ë¥¼ ì•Œë ¤ì£¼ì„¸ìš”"

#### ê³„í†µìš´ì˜ ê´€ë ¨
- "ê³„í†µìš´ì˜ì˜ ê¸°ë³¸ ì›ì¹™ì€ ë¬´ì—‡ì¸ê°€ìš”?"
- "ì†¡ì „ì œì•½ì´ë€ ë¬´ì—‡ì¸ê°€ìš”?"
- "ê³„í†µ ì•ˆì •ì„±ì„ ìœ„í•œ ì¡°ì¹˜ëŠ”?"

#### ì‹œì¥ìš´ì˜ ê´€ë ¨
- "ì „ë ¥ì‹œì¥ì˜ êµ¬ì¡°ëŠ” ì–´ë–»ê²Œ ë˜ë‚˜ìš”?"
- "ì…ì°° ì ˆì°¨ë¥¼ ì„¤ëª…í•´ì£¼ì„¸ìš”"
- "ê°€ê²© ê²°ì • ë°©ì‹ì€?"

---

## 8. API ì‚¬ìš©ë²•

### 8.1 ì£¼ìš” ì—”ë“œí¬ì¸íŠ¸

#### 1. ì§ˆë¬¸í•˜ê¸°
```bash
curl -X POST "http://localhost:8000/ask" \
     -H "Content-Type: application/json" \
     -d '{
       "question": "í•˜ë£¨ì „ë°œì „ê³„íšì´ ë¬´ì—‡ì¸ê°€ìš”?",
       "search_method": "hybrid"
     }'
```

**ì‘ë‹µ ì˜ˆì‹œ:**
```json
{
  "answer": "í•˜ë£¨ì „ë°œì „ê³„íšì€ ì „ë ¥ê±°ë˜ì¼ ì „ì¼ì— ìˆ˜ë¦½ë˜ëŠ” ë°œì „ê³„íšìœ¼ë¡œ...",
  "confidence": 0.92,
  "sources": ["ë°œì „ê³„íš_ê°€ì´ë“œ.md"],
  "reasoning": "ë„ë©”ì¸: ë°œì „ê³„íš, ì°¸ì¡° ë¬¸ì„œ ìˆ˜: 1",
  "search_results": 3,
  "search_method": "hybrid"
}
```

#### 2. ë¬¸ì„œ ê²€ìƒ‰
```bash
curl -X POST "http://localhost:8000/search" \
     -H "Content-Type: application/json" \
     -d '{
       "query": "ë°œì „ê³„íš",
       "method": "semantic",
       "top_k": 5
     }'
```

#### 3. ì‹œìŠ¤í…œ ìƒíƒœ í™•ì¸
```bash
curl -X GET "http://localhost:8000/status"
```

#### 4. íŒŒì¼ ì—…ë¡œë“œ
```bash
curl -X POST "http://localhost:8000/upload" \
     -H "accept: application/json" \
     -H "Content-Type: multipart/form-data" \
     -F "files=@document.pdf"
```

### 8.2 API ë¬¸ì„œ í™•ì¸
- **Swagger UI**: `http://localhost:8000/docs`
- **ReDoc**: `http://localhost:8000/redoc`

---

## 9. ë¬¸ì œí•´ê²°

### 9.1 ìì£¼ ë°œìƒí•˜ëŠ” ë¬¸ì œë“¤

#### 1. ë©”ëª¨ë¦¬ ë¶€ì¡± ì˜¤ë¥˜
```bash
# í•´ê²°ë°©ë²•: ì²­í¬ í¬ê¸° ì¤„ì´ê¸°
# config/config.yamlì—ì„œ CHUNK_SIZEë¥¼ 500ìœ¼ë¡œ ë³€ê²½
CHUNK_SIZE: 500
```

#### 2. ChromaDB ì˜¤ë¥˜
```bash
# í•´ê²°ë°©ë²•: ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™”
rm -rf vector_db/
python power_market_rag.py
```

#### 3. ëª¨ë¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨
```bash
# í•´ê²°ë°©ë²•: ìˆ˜ë™ ë‹¤ìš´ë¡œë“œ
python -c "
from sentence_transformers import SentenceTransformer
model = SentenceTransformer('paraphrase-multilingual-MiniLM-L12-v2')
"
```

#### 4. í•œê¸€ ì¸ì½”ë”© ë¬¸ì œ
```bash
# í•´ê²°ë°©ë²•: í™˜ê²½ë³€ìˆ˜ ì„¤ì •
export PYTHONIOENCODING=utf-8
export LANG=ko_KR.UTF-8
```

### 9.2 ì„±ëŠ¥ ìµœì í™” íŒ

#### 1. GPU ì‚¬ìš© (CUDA ì§€ì›ì‹œ)
```yaml
# config/config.yaml
EMBEDDING_MODEL: "sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2"
USE_GPU: true
```

#### 2. ë°°ì¹˜ í¬ê¸° ì¡°ì •
```yaml
# ë©”ëª¨ë¦¬ì— ë”°ë¼ ì¡°ì •
BATCH_SIZE: 16  # ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ 8ë¡œ ê°ì†Œ
```

#### 3. ì¸ë±ìŠ¤ ìµœì í™”
```python
# ì •ê¸°ì ì¸ ì¸ë±ìŠ¤ ë¦¬ë¹Œë“œ
python -c "
from power_market_rag import PowerMarketRAG
rag = PowerMarketRAG()
rag.clear_database()
rag.load_documents('documents')
"
```

---

## 10. ê³ ê¸‰ ì„¤ì •

### 10.1 zsh ìµœì í™” ì„¤ì •

```bash
# .zshrcì— ì¶”ê°€ ê°€ëŠ¥í•œ ë³„ì¹­
alias rag-start="cd /home/zwtiger/power_market_rag && source venv/bin/activate && ./run.sh"
alias rag-demo="cd /home/zwtiger/power_market_rag && source venv/bin/activate && python demo.py"
alias rag-test="cd /home/zwtiger/power_market_rag && source venv/bin/activate && python test_basic.py"
```

### 10.2 ì„±ëŠ¥ íŠœë‹

```yaml
# config/config.yamlì—ì„œ ì¡°ì • ê°€ëŠ¥
CHUNK_SIZE: 500          # ë©”ëª¨ë¦¬ ë¶€ì¡±ì‹œ ê°ì†Œ
EMBEDDING_MODEL: "multilingual-MiniLM-L12-v2"  # ë¹ ë¥¸ ì²˜ë¦¬
TOP_K: 3                 # ê²€ìƒ‰ ê²°ê³¼ ìˆ˜ ì¡°ì •
SIMILARITY_THRESHOLD: 0.8  # ì—„ê²©í•œ í•„í„°ë§
```

### 10.3 ê³ ê¸‰ ëª¨ë¸ ì„¤ì •

```python
# ë” ê°•ë ¥í•œ ì„ë² ë”© ëª¨ë¸ ì‚¬ìš©
EMBEDDING_MODEL: "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"

# OpenAI API ì—°ë™ (ì¶”í›„ êµ¬í˜„)
OPENAI_API_KEY: "your-api-key"
OPENAI_MODEL: "gpt-4"

# Claude API ì—°ë™ (ì¶”í›„ êµ¬í˜„)
CLAUDE_API_KEY: "your-api-key"
CLAUDE_MODEL: "claude-3-sonnet"
```

---

## ğŸ‰ ë§ˆë¬´ë¦¬

ì´ ë¬¸ì„œì—ëŠ” ì „ë ¥ì‹œì¥ RAG ì‹œìŠ¤í…œì˜ ëª¨ë“  êµ¬ì„± ìš”ì†Œê°€ í¬í•¨ë˜ì–´ ìˆìŠµë‹ˆë‹¤:

### âœ… ì™„ì„±ëœ ê¸°ëŠ¥ë“¤
- ğŸ“„ **ì™„ì „í•œ ë¬¸ì„œ ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸**
- ğŸ§  **ChromaDB ê¸°ë°˜ ë²¡í„° ì €ì¥ì†Œ**
- ğŸ” **4ê°€ì§€ ê²€ìƒ‰ ë°©ì‹** (ì˜ë¯¸ì , í‚¤ì›Œë“œ, í•˜ì´ë¸Œë¦¬ë“œ, ìŠ¤ë§ˆíŠ¸)
- ğŸ’¡ **ì „ë ¥ì‹œì¥ íŠ¹í™” ë‹µë³€ ìƒì„±**
- ğŸŒ **ì›¹ ì¸í„°í˜ì´ìŠ¤ ë° API**
- âš™ï¸ **ëª¨ë“ˆí˜• ì•„í‚¤í…ì²˜**
- ğŸ“š **ìƒì„¸í•œ ë¬¸ì„œí™”**

### ğŸš€ ì¦‰ì‹œ ì‹œì‘ ê°€ì´ë“œ
1. ì½”ë“œ ë³µì‚¬ í›„ íŒŒì¼ êµ¬ì¡° ìƒì„±
2. `chmod +x install.sh && ./install.sh` ì‹¤í–‰
3. `./run.sh` ì‹¤í–‰í•˜ì—¬ ì›¹ ì¸í„°í˜ì´ìŠ¤ ì‹œì‘
4. `http://localhost:8000`ì—ì„œ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸

ì´ì œ ì „ë ¥ì‹œì¥ ì „ë¬¸ ì§€ì‹ì„ ì¦‰ì‹œ ê²€ìƒ‰í•˜ê³  í™œìš©í•  ìˆ˜ ìˆëŠ” ì™„ì „í•œ RAG ì‹œìŠ¤í…œì„ ê°–ì¶”ê²Œ ë˜ì—ˆìŠµë‹ˆë‹¤! ğŸ¯