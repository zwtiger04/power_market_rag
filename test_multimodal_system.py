#!/usr/bin/env python3
"""
ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸
ì‹¤ì œ ì „ë ¥ì‹œì¥ ë¬¸ì„œë¡œ ì „ì²´ íŒŒì´í”„ë¼ì¸ ê²€ì¦
"""

import sys
import logging
from pathlib import Path
import time
import json
from typing import List, Dict, Any

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from core.multimodal_processor import get_multimodal_processor
from core.metadata_extractor import get_metadata_extractor
from core.vector_engine import get_vector_engine

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.StreamHandler(),
        logging.FileHandler('test_multimodal.log', encoding='utf-8')
    ]
)

logger = logging.getLogger(__name__)


class MultimodalSystemTester:
    """ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤í„°"""
    
    def __init__(self, data_dir: str = "data"):
        self.data_dir = Path(data_dir)
        self.documents_dir = self.data_dir / "documents"
        self.results_dir = self.data_dir / "test_results"
        
        # ê²°ê³¼ ë””ë ‰í† ë¦¬ ìƒì„±
        self.results_dir.mkdir(parents=True, exist_ok=True)
        
        # ì»´í¬ë„ŒíŠ¸ ì´ˆê¸°í™”
        self.multimodal_processor = get_multimodal_processor(data_dir)
        self.metadata_extractor = get_metadata_extractor(data_dir)
        self.vector_engine = get_vector_engine(data_dir)
        
        # í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥
        self.test_results = {
            "start_time": None,
            "end_time": None,
            "total_duration_ms": 0,
            "documents_tested": [],
            "component_performance": {},
            "errors": [],
            "summary": {}
        }
    
    def run_comprehensive_test(self, max_documents: int = 3) -> Dict[str, Any]:
        """í¬ê´„ì ì¸ ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸ ì‹¤í–‰"""
        logger.info("ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
        self.test_results["start_time"] = time.time()
        
        try:
            # 1. ì˜ì¡´ì„± í™•ì¸
            self._check_dependencies()
            
            # 2. ë¬¸ì„œ ì„ íƒ
            test_documents = self._select_test_documents(max_documents)
            
            # 3. ê° ë¬¸ì„œë³„ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸
            for doc_path in test_documents:
                self._test_document_pipeline(doc_path)
            
            # 4. ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸
            self._test_search_performance()
            
            # 5. ê²°ê³¼ ì •ë¦¬ ë° ì €ì¥
            self._finalize_test_results()
            
            logger.info("ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì™„ë£Œ")
            return self.test_results
            
        except Exception as e:
            logger.error(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì¤‘ ì˜¤ë¥˜: {e}")
            self.test_results["errors"].append(str(e))
            return self.test_results
    
    def _check_dependencies(self):
        """ì˜ì¡´ì„± í™•ì¸"""
        logger.info("ì‹œìŠ¤í…œ ì˜ì¡´ì„± í™•ì¸ ì¤‘...")
        
        dependencies = {
            "multimodal_processor": self.multimodal_processor is not None,
            "metadata_extractor": self.metadata_extractor is not None,
            "vector_engine": self.vector_engine is not None,
            "documents_dir_exists": self.documents_dir.exists(),
            "has_pdf_files": len(list(self.documents_dir.glob("*.pdf"))) > 0
        }
        
        for dep, status in dependencies.items():
            if not status:
                raise Exception(f"ì˜ì¡´ì„± í™•ì¸ ì‹¤íŒ¨: {dep}")
        
        logger.info("ëª¨ë“  ì˜ì¡´ì„± í™•ì¸ ì™„ë£Œ")
    
    def _select_test_documents(self, max_documents: int) -> List[Path]:
        """í…ŒìŠ¤íŠ¸í•  ë¬¸ì„œ ì„ íƒ"""
        pdf_files = list(self.documents_dir.glob("*.pdf"))
        
        # ì‘ì€ ë¬¸ì„œë¶€í„° ì„ íƒ (í…ŒìŠ¤íŠ¸ íš¨ìœ¨ì„±ì„ ìœ„í•´)
        pdf_files_with_size = [(f, f.stat().st_size) for f in pdf_files]
        pdf_files_with_size.sort(key=lambda x: x[1])  # í¬ê¸°ìˆœ ì •ë ¬
        
        selected = [f[0] for f in pdf_files_with_size[:max_documents]]
        
        logger.info(f"í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ì„ íƒ: {[f.name for f in selected]}")
        return selected
    
    def _test_document_pipeline(self, doc_path: Path):
        """ë‹¨ì¼ ë¬¸ì„œì— ëŒ€í•œ ì „ì²´ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸"""
        logger.info(f"ë¬¸ì„œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹œì‘: {doc_path.name}")
        
        doc_result = {
            "document_path": str(doc_path),
            "document_name": doc_path.name,
            "file_size_bytes": doc_path.stat().st_size,
            "stages": {},
            "total_processing_time_ms": 0,
            "success": False,
            "error": None
        }
        
        start_time = time.time()
        
        try:
            # Stage 1: Multimodal Processing
            stage_start = time.time()
            logger.info(f"Stage 1: ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ - {doc_path.name}")
            
            processed_doc = self.multimodal_processor.process_document(doc_path)
            
            stage_duration = (time.time() - stage_start) * 1000
            doc_result["stages"]["multimodal_processing"] = {
                "duration_ms": stage_duration,
                "success": "error" not in processed_doc,
                "sections_extracted": len(processed_doc.get("content", {}).get("sections", [])),
                "paragraphs_extracted": len(processed_doc.get("content", {}).get("paragraphs", [])),
                "images_extracted": len(processed_doc.get("multimodal_content", {}).get("images", [])),
                "tables_extracted": len(processed_doc.get("multimodal_content", {}).get("tables", [])),
                "formulas_extracted": len(processed_doc.get("multimodal_content", {}).get("formulas", []))
            }
            
            if "error" in processed_doc:
                raise Exception(f"ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ ì‹¤íŒ¨: {processed_doc['error']}")
            
            # Stage 2: Metadata Extraction
            stage_start = time.time()
            logger.info(f"Stage 2: ë©”íƒ€ë°ì´í„° ì¶”ì¶œ - {doc_path.name}")
            
            metadata = self.metadata_extractor.extract_metadata(processed_doc)
            
            stage_duration = (time.time() - stage_start) * 1000
            doc_result["stages"]["metadata_extraction"] = {
                "duration_ms": stage_duration,
                "success": "error" not in metadata,
                "entities_extracted": self._count_entities(metadata),
                "keywords_extracted": len(metadata.get("content_metadata", {}).get("keyword_analysis", {}).get("top_keywords", [])),
                "relationships_found": len(metadata.get("relationships", [])),
                "quality_score": metadata.get("quality_indicators", {}).get("overall_quality", 0)
            }
            
            if "error" in metadata:
                raise Exception(f"ë©”íƒ€ë°ì´í„° ì¶”ì¶œ ì‹¤íŒ¨: {metadata['error']}")
            
            # Stage 3: Vector Engine Integration
            stage_start = time.time()
            logger.info(f"Stage 3: ë²¡í„° ì—”ì§„ í†µí•© - {doc_path.name}")
            
            # ë¬¸ì„œë¥¼ ë²¡í„° ì—”ì§„ì— ì¶”ê°€
            doc_id = processed_doc["document_id"]
            content = processed_doc["content"]
            doc_metadata = {
                "file_name": doc_path.name,
                "file_size": doc_path.stat().st_size,
                "processing_date": processed_doc["processed_at"],
                **metadata.get("power_market_metadata", {}).get("document_classification", {})
            }
            
            vector_success = self.vector_engine.add_document(doc_id, content, doc_metadata)
            
            stage_duration = (time.time() - stage_start) * 1000
            doc_result["stages"]["vector_integration"] = {
                "duration_ms": stage_duration,
                "success": vector_success,
                "document_added": vector_success
            }
            
            if not vector_success:
                raise Exception("ë²¡í„° ì—”ì§„ í†µí•© ì‹¤íŒ¨")
            
            # ì „ì²´ ì„±ê³µ
            doc_result["success"] = True
            doc_result["total_processing_time_ms"] = (time.time() - start_time) * 1000
            
            # ê²°ê³¼ ì €ì¥
            self._save_document_results(doc_id, processed_doc, metadata)
            
            logger.info(f"ë¬¸ì„œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì„±ê³µ: {doc_path.name} ({doc_result['total_processing_time_ms']:.2f}ms)")
            
        except Exception as e:
            doc_result["error"] = str(e)
            doc_result["total_processing_time_ms"] = (time.time() - start_time) * 1000
            logger.error(f"ë¬¸ì„œ íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {doc_path.name} - {e}")
        
        self.test_results["documents_tested"].append(doc_result)
    
    def _test_search_performance(self):
        """ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸"""
        logger.info("ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì‹œì‘")
        
        search_queries = [
            "ì „ë ¥ì‹œì¥ ìš´ì˜",
            "ë°œì „ì‚¬ì—…ì",
            "ì‹œìŠ¤í…œí•œê³„ê°€ê²©",
            "ê¸‰ì „ì§€ì‹œ",
            "ë³´ì¡°ì„œë¹„ìŠ¤",
            "ì •ì‚° ì ˆì°¨"
        ]
        
        search_results = {
            "queries_tested": len(search_queries),
            "total_search_time_ms": 0,
            "average_search_time_ms": 0,
            "successful_searches": 0,
            "query_results": []
        }
        
        start_time = time.time()
        
        for query in search_queries:
            query_start = time.time()
            
            try:
                result = self.vector_engine.search(
                    query=query,
                    search_type="hybrid",
                    level="all",
                    top_k=5,
                    ai_friendly=True
                )
                
                query_duration = (time.time() - query_start) * 1000
                
                query_result = {
                    "query": query,
                    "duration_ms": query_duration,
                    "success": "error" not in result,
                    "results_count": len(result.get("primary_results", [])),
                    "total_results": result.get("search_metadata", {}).get("total_results", 0)
                }
                
                if "error" not in result:
                    search_results["successful_searches"] += 1
                
                search_results["query_results"].append(query_result)
                
                logger.info(f"ê²€ìƒ‰ ì¿¼ë¦¬ '{query}': {query_duration:.2f}ms, {query_result['total_results']}ê°œ ê²°ê³¼")
                
            except Exception as e:
                logger.error(f"ê²€ìƒ‰ ì¿¼ë¦¬ '{query}' ì‹¤íŒ¨: {e}")
        
        search_results["total_search_time_ms"] = (time.time() - start_time) * 1000
        search_results["average_search_time_ms"] = search_results["total_search_time_ms"] / len(search_queries)
        
        self.test_results["component_performance"]["search"] = search_results
        
        logger.info(f"ê²€ìƒ‰ ì„±ëŠ¥ í…ŒìŠ¤íŠ¸ ì™„ë£Œ: í‰ê·  {search_results['average_search_time_ms']:.2f}ms")
    
    def _count_entities(self, metadata: Dict[str, Any]) -> int:
        """ë©”íƒ€ë°ì´í„°ì—ì„œ ì¶”ì¶œëœ ì—”í‹°í‹° ìˆ˜ ê³„ì‚°"""
        entities = metadata.get("power_market_metadata", {}).get("market_entities", {})
        return sum(len(entity_list) for entity_list in entities.values())
    
    def _save_document_results(self, doc_id: str, processed_doc: Dict[str, Any], metadata: Dict[str, Any]):
        """ë¬¸ì„œë³„ ê²°ê³¼ ì €ì¥"""
        try:
            # ì²˜ë¦¬ëœ ë¬¸ì„œ ì €ì¥
            self.multimodal_processor.save_processed_document(processed_doc)
            
            # ë©”íƒ€ë°ì´í„° ì €ì¥
            self.metadata_extractor.save_metadata(metadata)
            
            # ë²¡í„° ì—”ì§„ ë©”íƒ€ë°ì´í„° ì €ì¥
            self.vector_engine.save_metadata()
            
        except Exception as e:
            logger.warning(f"ë¬¸ì„œ ê²°ê³¼ ì €ì¥ ì‹¤íŒ¨ {doc_id}: {e}")
    
    def _finalize_test_results(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì •ë¦¬"""
        self.test_results["end_time"] = time.time()
        self.test_results["total_duration_ms"] = (
            self.test_results["end_time"] - self.test_results["start_time"]
        ) * 1000
        
        # ì„±ê³µë¥  ê³„ì‚°
        total_docs = len(self.test_results["documents_tested"])
        successful_docs = len([d for d in self.test_results["documents_tested"] if d["success"]])
        
        # ì»´í¬ë„ŒíŠ¸ë³„ í†µê³„
        multimodal_stats = self.multimodal_processor.get_processing_stats()
        metadata_stats = self.metadata_extractor.get_extraction_stats()
        vector_stats = self.vector_engine.get_statistics()
        
        self.test_results["summary"] = {
            "total_documents": total_docs,
            "successful_documents": successful_docs,
            "success_rate": (successful_docs / total_docs * 100) if total_docs > 0 else 0,
            "total_processing_time_ms": self.test_results["total_duration_ms"],
            "average_processing_time_per_doc_ms": (
                self.test_results["total_duration_ms"] / total_docs
            ) if total_docs > 0 else 0,
            "component_stats": {
                "multimodal_processor": multimodal_stats,
                "metadata_extractor": metadata_stats,
                "vector_engine": vector_stats
            }
        }
        
        # ê²°ê³¼ íŒŒì¼ë¡œ ì €ì¥
        results_file = self.results_dir / f"test_results_{int(time.time())}.json"
        with open(results_file, "w", encoding="utf-8") as f:
            json.dump(self.test_results, f, ensure_ascii=False, indent=2, default=str)
        
        logger.info(f"í…ŒìŠ¤íŠ¸ ê²°ê³¼ ì €ì¥: {results_file}")
    
    def print_summary(self):
        """í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½ ì¶œë ¥"""
        summary = self.test_results.get("summary", {})
        
        print("\n" + "="*80)
        print("ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ê²°ê³¼ ìš”ì•½")
        print("="*80)
        
        print(f"ğŸ“Š ì „ì²´ í†µê³„:")
        print(f"  - í…ŒìŠ¤íŠ¸ ë¬¸ì„œ ìˆ˜: {summary.get('total_documents', 0)}")
        print(f"  - ì„±ê³µí•œ ë¬¸ì„œ ìˆ˜: {summary.get('successful_documents', 0)}")
        print(f"  - ì„±ê³µë¥ : {summary.get('success_rate', 0):.1f}%")
        print(f"  - ì „ì²´ ì²˜ë¦¬ ì‹œê°„: {summary.get('total_processing_time_ms', 0):.2f}ms")
        print(f"  - ë¬¸ì„œë‹¹ í‰ê·  ì²˜ë¦¬ ì‹œê°„: {summary.get('average_processing_time_per_doc_ms', 0):.2f}ms")
        
        print(f"\nğŸ”§ ì»´í¬ë„ŒíŠ¸ë³„ ì„±ëŠ¥:")
        component_stats = summary.get("component_stats", {})
        
        # ë©€í‹°ëª¨ë‹¬ í”„ë¡œì„¸ì„œ
        mm_stats = component_stats.get("multimodal_processor", {})
        print(f"  ğŸ“„ ë©€í‹°ëª¨ë‹¬ í”„ë¡œì„¸ì„œ:")
        print(f"    - ì²˜ë¦¬ëœ ë¬¸ì„œ: {mm_stats.get('processed_documents', 0)}")
        print(f"    - ì¶”ì¶œëœ ì´ë¯¸ì§€: {mm_stats.get('extracted_images', 0)}")
        print(f"    - ì¶”ì¶œëœ í‘œ: {mm_stats.get('extracted_tables', 0)}")
        print(f"    - ì¶”ì¶œëœ ìˆ˜ì‹: {mm_stats.get('extracted_formulas', 0)}")
        
        # ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°
        md_stats = component_stats.get("metadata_extractor", {})
        print(f"  ğŸ·ï¸  ë©”íƒ€ë°ì´í„° ì¶”ì¶œê¸°:")
        print(f"    - ì²˜ë¦¬ëœ ë¬¸ì„œ: {md_stats.get('documents_processed', 0)}")
        print(f"    - ì¶”ì¶œëœ ì—”í‹°í‹°: {md_stats.get('entities_extracted', 0)}")
        print(f"    - ì¶”ì¶œëœ í‚¤ì›Œë“œ: {md_stats.get('keywords_extracted', 0)}")
        print(f"    - ë§¤í•‘ëœ ê´€ê³„: {md_stats.get('relations_mapped', 0)}")
        
        # ë²¡í„° ì—”ì§„
        ve_stats = component_stats.get("vector_engine", {})
        print(f"  ğŸ” ë²¡í„° ì—”ì§„:")
        collections = ve_stats.get("collections", {})
        for collection_name, collection_info in collections.items():
            print(f"    - {collection_name}: {collection_info.get('count', 0)}ê°œ í•­ëª©")
        
        # ê²€ìƒ‰ ì„±ëŠ¥
        search_stats = self.test_results.get("component_performance", {}).get("search", {})
        if search_stats:
            print(f"  ğŸ” ê²€ìƒ‰ ì„±ëŠ¥:")
            print(f"    - í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ìˆ˜: {search_stats.get('queries_tested', 0)}")
            print(f"    - ì„±ê³µí•œ ê²€ìƒ‰: {search_stats.get('successful_searches', 0)}")
            print(f"    - í‰ê·  ê²€ìƒ‰ ì‹œê°„: {search_stats.get('average_search_time_ms', 0):.2f}ms")
        
        # ì˜¤ë¥˜ ì •ë³´
        errors = self.test_results.get("errors", [])
        if errors:
            print(f"\nâŒ ì˜¤ë¥˜ ({len(errors)}ê°œ):")
            for i, error in enumerate(errors[:5], 1):  # ìµœëŒ€ 5ê°œë§Œ í‘œì‹œ
                print(f"  {i}. {error}")
            if len(errors) > 5:
                print(f"  ... ë° {len(errors) - 5}ê°œ ì¶”ê°€ ì˜¤ë¥˜")
        
        print("\n" + "="*80)


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ë©€í‹°ëª¨ë‹¬ ì‹œìŠ¤í…œ í†µí•© í…ŒìŠ¤íŠ¸ ì‹œì‘")
    
    try:
        # í…ŒìŠ¤í„° ì´ˆê¸°í™”
        tester = MultimodalSystemTester()
        
        # í…ŒìŠ¤íŠ¸ ì‹¤í–‰ (ìµœëŒ€ 3ê°œ ë¬¸ì„œë¡œ ì œí•œ)
        results = tester.run_comprehensive_test(max_documents=3)
        
        # ê²°ê³¼ ì¶œë ¥
        tester.print_summary()
        
        return results
        
    except Exception as e:
        print(f"í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        logger.error(f"ë©”ì¸ í…ŒìŠ¤íŠ¸ ì‹¤í–‰ ì‹¤íŒ¨: {e}")
        return None


if __name__ == "__main__":
    main()